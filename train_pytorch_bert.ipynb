{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts, model, tokeniser, num_hiddens=4, maxlen=128):\n",
    "    inputs = tokeniser(texts, padding=True, truncation=True, max_length=maxlen, pad_to_multiple_of=maxlen, return_tensors='pt')\n",
    "    inputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\n",
    "    inputs['attention_mask'] = inputs['attention_mask'].to(\"cuda\")\n",
    "    inputs['token_type_ids'] = inputs['token_type_ids'].to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return torch.cat([layer.unsqueeze(0) for layer in outputs[2][-4:]]).mean(dim=[0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.89 ms ± 44.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "embed_texts(text_batch, model, tokenizer, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = [\"what's up with you today ?\", \"hello world !\", \"how are you doing today ?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model = BertModel.from_pretrained('bert-base-uncased', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text_batch, padding=True, truncation=True, max_length=50, pad_to_multiple_of=50, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = torch.cat([layer.unsqueeze(0) for layer in outputs[2][-4:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.6976e-01,  1.0468e-01, -1.4612e-01,  ..., -5.6408e-01,\n",
       "           4.1560e-01, -2.7133e-02],\n",
       "         [ 7.5136e-01, -4.9792e-01,  1.6496e-01,  ...,  3.3014e-01,\n",
       "           2.5959e-01, -5.4828e-01],\n",
       "         [ 1.1475e+00, -3.9137e-01,  9.5390e-01,  ..., -4.2770e-01,\n",
       "           2.9030e-01, -5.4206e-01],\n",
       "         ...,\n",
       "         [ 3.1620e-01, -1.6403e-01, -7.0130e-04,  ...,  7.5811e-02,\n",
       "           1.6194e-01, -4.7568e-01],\n",
       "         [-4.1781e-02, -3.6421e-01, -6.5790e-01,  ...,  3.7247e-01,\n",
       "           5.2277e-01, -4.3751e-01],\n",
       "         [ 1.1585e-01, -3.4933e-01, -5.8629e-01,  ...,  3.6592e-01,\n",
       "           3.8491e-01, -6.0045e-01]],\n",
       "\n",
       "        [[-1.4241e-01,  1.3354e-01, -1.2907e-01,  ..., -3.5968e-01,\n",
       "          -5.6223e-02,  3.6050e-01],\n",
       "         [-3.5065e-01,  1.0420e-01,  6.2445e-01,  ..., -1.7610e-01,\n",
       "           4.8340e-01,  6.4435e-02],\n",
       "         [-2.4513e-01, -1.5732e-01,  6.9452e-01,  ..., -5.6545e-01,\n",
       "          -8.9399e-02, -1.8564e-01],\n",
       "         ...,\n",
       "         [-3.7155e-01, -1.9050e-01,  6.2676e-01,  ..., -9.8793e-02,\n",
       "          -2.6767e-02, -1.2832e-01],\n",
       "         [-1.9831e-01, -2.5011e-01,  5.4220e-01,  ..., -1.2138e-01,\n",
       "          -4.1617e-02, -1.2599e-01],\n",
       "         [-5.5836e-01, -3.7466e-01,  2.0726e-01,  ...,  1.3989e-01,\n",
       "          -1.5269e-01, -4.2430e-01]],\n",
       "\n",
       "        [[ 5.7587e-02,  2.0130e-02, -4.1563e-01,  ..., -6.0554e-01,\n",
       "           2.9214e-01,  1.8027e-02],\n",
       "         [ 3.2850e-01, -7.0807e-01, -6.1148e-01,  ..., -1.3815e-01,\n",
       "           7.4971e-01, -8.2085e-01],\n",
       "         [ 7.0882e-01, -6.4730e-01,  8.5831e-02,  ..., -8.1305e-01,\n",
       "           3.0061e-01, -8.2485e-01],\n",
       "         ...,\n",
       "         [ 1.4131e-01, -5.5040e-01, -2.6019e-01,  ...,  3.5075e-01,\n",
       "           4.0871e-01, -4.4509e-01],\n",
       "         [ 3.0694e-01, -2.3587e-01,  9.8530e-02,  ...,  7.8153e-02,\n",
       "           7.9382e-02, -3.8232e-01],\n",
       "         [ 2.7242e-01, -2.3414e-01,  1.9104e-01,  ...,  6.0345e-02,\n",
       "          -1.6227e-02, -3.1256e-01]]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2][-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 50, 768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.mean(dim=[0,2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7253e-01, -2.7076e-01, -6.5027e-02, -1.3390e-01,  1.5993e-01,\n",
       "        -5.8235e-01,  2.6562e-01,  5.3476e-01, -2.4954e-01,  2.4915e-03,\n",
       "         2.6613e-01, -2.2889e-01, -1.6627e-01,  2.9992e-01, -1.8342e-01,\n",
       "         3.9602e-01,  4.6814e-01,  2.6224e-01, -2.0788e-01,  3.9021e-01,\n",
       "        -6.9101e-02,  7.9177e-02, -1.2817e-01,  1.5459e-01,  2.7015e-01,\n",
       "        -1.2630e-01, -4.4452e-01, -3.1085e-02,  8.1686e-02,  5.3798e-02,\n",
       "         1.4821e-01,  6.4809e-02, -8.3380e-02,  1.2476e-01, -2.7359e-02,\n",
       "         1.6790e-01, -2.4641e-01, -2.5840e-01, -2.3039e-01, -1.7364e-02,\n",
       "        -9.5702e-01, -6.5898e-01, -3.1682e-01, -6.8927e-02, -5.2936e-02,\n",
       "        -2.2041e-01,  5.4136e-01, -4.7485e-01, -3.3888e-01, -2.1948e-01,\n",
       "         2.8569e-01,  4.7347e-01, -2.2099e-01,  1.2541e-01, -5.0365e-01,\n",
       "        -3.2506e-02,  6.0827e-02, -8.4027e-01, -4.3488e-01, -1.2858e-01,\n",
       "         8.2173e-02, -1.4421e-01,  3.2299e-02,  1.5845e-02,  3.6986e-01,\n",
       "        -1.9851e-02,  2.6712e-01,  6.1869e-02, -3.2095e-01,  7.1217e-01,\n",
       "        -7.3911e-01, -1.8717e-01, -4.3663e-02, -1.8394e-01,  2.8617e-01,\n",
       "         3.0952e-01, -4.1331e-02,  2.6308e-01, -1.8372e-01,  8.7624e-02,\n",
       "         1.1521e-01,  2.1158e-01,  1.7786e-01,  4.5911e-01, -3.1696e-01,\n",
       "        -7.9266e-02, -6.7600e-01,  3.7688e-01,  1.0503e-01,  6.0357e-01,\n",
       "        -1.0003e-01, -1.1091e-01,  4.6669e-01,  6.9025e-01,  2.3460e-01,\n",
       "        -3.6095e-01, -2.7496e-01,  6.2698e-02,  1.2109e-01, -1.4036e-01,\n",
       "        -1.3003e-01, -2.9198e-01,  3.4341e-01,  1.9157e-01, -5.1537e-01,\n",
       "        -1.6470e-01,  4.4898e-01, -7.9489e-01,  2.1886e-01,  4.0419e-01,\n",
       "        -3.9497e-01, -2.7605e-01,  7.3352e-02, -2.3252e-01, -1.9065e-01,\n",
       "         4.0704e-01,  1.7100e-01, -2.9019e-01, -3.8281e-01,  1.3616e-01,\n",
       "        -1.7374e-01, -4.7719e-01, -4.2618e-02,  6.0247e-01, -6.0558e-02,\n",
       "         4.1856e-01, -5.5465e-01, -6.4602e-01,  2.0899e-01, -6.7809e-01,\n",
       "         4.5720e-01,  3.5087e-01,  3.5939e-01,  1.2145e-01, -6.6332e-01,\n",
       "         3.6094e-01,  1.5547e-01, -1.2689e-01, -1.3690e-01, -3.0103e-01,\n",
       "        -6.1810e-02,  2.3044e-01,  3.8630e-01,  9.4377e-03, -1.1911e-01,\n",
       "         3.3218e-02, -4.2291e-01, -3.1710e-01,  2.8835e-01, -4.6802e-01,\n",
       "        -2.3291e-01, -7.2863e-02, -2.2401e-01, -3.5134e-01, -2.7128e-01,\n",
       "         1.8005e-03, -7.7080e-01, -2.9391e-02, -1.6876e-01,  9.3183e-02,\n",
       "         4.7032e-01, -1.0917e-01, -2.5968e-01,  8.1259e-01,  3.0159e-01,\n",
       "         5.6287e-02,  4.5432e-01,  2.9429e-01,  8.9138e-02,  1.9217e-01,\n",
       "        -3.0353e-02,  3.1537e-01,  8.2033e-01,  9.8311e-03, -5.8296e-01,\n",
       "         1.5638e-01,  5.7338e-01,  3.6035e-01, -2.5886e-01,  5.0724e-01,\n",
       "        -3.2728e-01,  1.1270e-01, -1.0322e-01, -7.2349e-03, -2.9637e-02,\n",
       "         2.1168e-01,  5.7556e-01, -7.0723e-02, -2.1808e-01,  2.0519e-01,\n",
       "        -1.3199e-01, -2.5068e-01, -1.0207e-01,  1.4926e-01,  2.1813e-01,\n",
       "        -8.2096e-01, -7.4993e-02, -2.3219e-01,  1.5171e-03, -1.8486e-01,\n",
       "         2.3800e-01, -4.7612e-01,  8.9467e-03,  2.8128e-01, -2.4812e-01,\n",
       "        -2.5301e-01,  3.2646e-01, -1.9526e-02, -4.8716e-01,  2.3075e-02,\n",
       "        -7.6051e-01, -6.9768e-02,  1.6011e-02,  7.9448e-01, -3.6652e-02,\n",
       "        -9.2675e-02,  3.4932e-01,  1.6521e-01, -2.7485e-01,  1.1143e-02,\n",
       "         4.0919e-01, -5.8982e-01, -1.1801e-01,  1.1029e-02, -2.5481e-01,\n",
       "         1.8834e-01,  7.9313e-01,  7.1857e-02,  5.1415e-02, -4.2090e-02,\n",
       "        -2.9403e-02,  1.4055e-01, -2.5571e-01, -1.1632e-01, -6.6557e-01,\n",
       "        -6.3587e-02, -3.2794e-02, -3.3183e-01, -1.4185e-02, -4.9783e-01,\n",
       "        -4.4831e-01,  5.0878e-02,  6.3708e-01,  3.7832e-01, -1.1484e-01,\n",
       "         2.2288e-01,  1.7210e-01, -5.3350e-03,  4.8905e-02, -1.2581e-01,\n",
       "        -4.8410e-01, -2.5833e-01, -2.8194e-02,  1.7001e-01, -5.0874e-01,\n",
       "         2.9009e-02, -1.6265e-01,  3.9735e-01,  3.4879e-01,  9.2885e-02,\n",
       "         1.9083e-01, -8.9280e-03,  5.1573e-01,  2.3009e-01,  1.5349e-01,\n",
       "         1.9570e-01,  4.0123e-01, -3.0088e-01, -7.7103e-02, -6.3986e-02,\n",
       "         3.9359e-01,  1.0999e-02,  3.1619e-01,  4.9335e-01, -3.7605e-04,\n",
       "        -5.5321e-01,  3.4579e-01, -3.6393e-01, -3.8309e-02,  2.8629e-01,\n",
       "        -5.5812e-03,  3.8202e-01, -8.9245e-01, -1.4029e-01, -5.4845e-02,\n",
       "        -3.5445e-01,  1.1346e-01,  5.8968e-02, -9.2806e-02, -6.6233e-02,\n",
       "         3.2891e-02, -1.0074e-01, -2.3860e-01,  1.9074e-01,  5.5054e-01,\n",
       "        -5.0947e-02,  6.1674e-01, -1.3636e-01,  2.5548e-01,  8.3944e-02,\n",
       "         1.5202e-01, -7.6283e-02,  6.6902e-02,  7.3383e-02, -6.4988e-02,\n",
       "         1.3757e-01, -2.9803e-02, -1.1178e-01, -4.3942e+00,  4.1892e-02,\n",
       "         5.0932e-01,  2.3052e-01, -5.8931e-01,  5.6139e-02, -1.2019e-01,\n",
       "        -1.7903e-01, -4.4310e-01, -1.6790e-01, -5.0542e-02, -3.5029e-02,\n",
       "         5.0224e-02,  3.2619e-01,  1.1411e-01, -2.9392e-01,  5.4824e-01,\n",
       "        -7.6772e-01, -2.7746e-01,  5.3679e-01, -5.8548e-02,  8.8645e-02,\n",
       "         5.3357e-03, -2.7083e-01,  1.6907e-01,  7.3186e-01,  4.7195e-01,\n",
       "         3.1103e-01, -4.8413e-01,  1.4428e-01, -1.4935e-01, -8.3680e-02,\n",
       "         1.5044e-01,  1.5379e-01, -6.5650e-02,  1.3202e-02, -2.3210e-02,\n",
       "        -3.8478e-01,  3.6117e-01,  3.0408e-01,  4.2007e-01, -4.5686e-01,\n",
       "         1.3527e-02, -3.4450e-02,  4.2390e-01,  5.7565e-02,  6.4638e-02,\n",
       "        -4.7916e-01, -1.3196e-01,  2.1096e-01,  4.2604e-01,  3.7725e-02,\n",
       "        -1.6914e-01,  2.7082e-02, -2.6946e-01,  3.3471e-01,  1.0850e-01,\n",
       "         3.9150e-01, -3.5376e-02, -8.8149e-02,  5.9074e-01, -2.0601e-01,\n",
       "         5.4221e-02,  1.1299e-01, -3.5160e-01, -7.9826e-02, -4.9760e-01,\n",
       "        -4.3804e-01,  2.0404e-01,  2.4551e-01, -2.8057e-01, -1.6232e-01,\n",
       "        -1.7221e-01, -4.6261e-01, -3.8520e-01, -1.6183e-02, -1.7929e-01,\n",
       "         1.6279e-01,  8.2160e-02, -2.1904e-01, -2.5591e-01, -2.7715e-01,\n",
       "         1.0353e-01, -6.0234e-02, -1.6419e-01, -7.7178e-01,  4.5772e-01,\n",
       "        -5.1403e-01, -6.8637e-02,  5.7260e-01,  5.3808e-01,  9.8810e-02,\n",
       "         2.5105e-01,  1.8153e-01,  3.7685e-01, -3.7018e-01,  2.1134e-01,\n",
       "        -2.8789e-01, -1.0016e-01,  3.6917e-01, -2.0313e-01, -2.7713e-01,\n",
       "         3.2633e-01,  6.7316e-01,  3.7773e-01,  6.6030e-02, -7.2847e-01,\n",
       "         5.2215e-02,  2.7587e-01,  1.8344e-01,  2.8343e-01, -4.8273e-01,\n",
       "         2.0469e-01, -3.9954e-01,  5.2038e-02, -2.7443e-01,  3.3433e-01,\n",
       "         3.6790e-01, -1.7956e-02,  4.1600e-01, -2.8646e-01,  5.9211e-01,\n",
       "        -2.6448e-01,  1.2452e-01, -8.0511e-02, -7.2790e-02, -4.1715e-01,\n",
       "        -6.2816e-01,  5.0550e-01,  4.1287e-01,  3.3658e-01, -3.1406e-01,\n",
       "        -3.4208e-01, -2.2105e-01, -5.5190e-02,  1.2775e-01, -2.3615e-02,\n",
       "        -2.9530e-01, -4.5827e-01,  1.1547e-01,  2.0657e-01, -5.1419e-02,\n",
       "        -3.1071e-01, -2.1614e-02,  1.9741e-01,  4.7724e-01, -9.9426e-02,\n",
       "        -4.0778e-01, -1.2471e-01,  5.9739e-01,  1.8882e-01, -5.3807e-01,\n",
       "        -1.5625e-01,  3.9214e-01, -3.2099e-01,  1.5283e-01,  3.5932e-01,\n",
       "        -9.5194e-02, -4.0315e-02, -2.6632e-01,  1.2210e-01, -3.0961e-02,\n",
       "         3.8758e-01,  3.9137e-01,  3.4201e-01,  4.4049e-01, -1.9927e-02,\n",
       "         1.5834e-01, -4.1054e-01, -5.2004e-01, -3.9820e-02,  1.1363e-01,\n",
       "         9.0552e-03, -8.5121e-02, -1.4979e-01,  2.0963e-01, -7.5239e-02,\n",
       "        -4.2219e-01,  6.8268e-02,  9.1854e-02, -1.1493e-01, -4.7670e-01,\n",
       "        -1.1817e-01,  3.8820e-02,  2.9744e-02,  9.9665e-02,  2.1606e-01,\n",
       "         1.0910e-01,  4.5680e-02,  2.8053e-01, -1.8909e-02,  3.4733e-01,\n",
       "         1.5540e-01, -2.3145e-01, -2.0815e-01, -4.4442e-01,  2.4432e-01,\n",
       "        -6.1302e-03,  4.9070e-01,  3.4020e-01, -2.5094e-01, -2.5528e-01,\n",
       "         1.2663e-01,  2.2778e-02,  1.3134e-01,  1.6940e-01,  1.3103e-01,\n",
       "         8.2271e-02, -1.0679e-01, -3.3457e-03,  1.0947e-01, -3.5037e-01,\n",
       "        -1.3833e-01, -5.5719e-02, -2.1112e-01,  1.7014e-01,  1.8956e-01,\n",
       "        -1.6766e-01,  2.4812e-01,  5.2128e-02, -5.9586e-01,  2.2730e-01,\n",
       "         2.5556e-02, -3.4449e-01, -4.8939e-01, -2.9390e-01, -4.6810e-01,\n",
       "        -1.7261e-01, -2.4726e-01, -2.0107e-01,  3.6067e-01, -6.2493e-02,\n",
       "         1.0006e-01,  7.4476e-01,  6.2647e-04,  5.2960e-01, -6.6436e-01,\n",
       "         5.5375e-02, -1.9768e-01,  5.9571e-01,  3.0190e-01, -1.2772e-01,\n",
       "         2.8943e-01, -2.3986e-01, -2.8130e-02,  8.9152e-02,  3.0891e-01,\n",
       "         1.8651e-01,  2.5251e-01, -3.2790e-01,  3.0119e-02,  4.2384e-02,\n",
       "         2.2719e-01, -1.3139e-01,  2.0414e-01, -5.5738e-01, -6.8230e-01,\n",
       "        -2.5759e-01, -3.6116e-01,  1.1278e-02, -4.4997e-02, -1.4350e-01,\n",
       "        -2.4645e-01, -4.1731e-01, -4.7443e-01, -5.4569e-01, -3.8437e-01,\n",
       "         2.3746e-01,  4.8261e-01, -6.0013e-02, -8.7935e-01, -1.4176e-01,\n",
       "        -1.1998e-01,  1.2669e-01, -1.3545e-01, -3.7304e-01,  4.5444e-01,\n",
       "        -7.7256e-01, -2.9708e-01,  2.2347e-01, -1.4158e-01,  3.3852e-01,\n",
       "        -2.8118e-01, -1.6567e-01,  4.1006e-01, -4.0505e-01,  3.7801e-02,\n",
       "        -1.2271e-01, -5.1214e-02,  1.2880e-01, -1.6206e-01,  2.4558e-01,\n",
       "         4.4598e-02,  3.4898e-01,  4.5191e-01,  1.2225e-01,  1.2954e-01,\n",
       "         3.9421e-01, -2.0599e-01, -3.0936e-01, -5.7504e-01,  2.6225e-01,\n",
       "         2.4338e-01, -4.6621e-02,  1.6624e-01,  2.9478e-01, -3.0769e-02,\n",
       "        -3.3520e-04,  1.0571e-01,  2.9725e-01, -6.4990e-02,  1.1514e-01,\n",
       "         1.2021e-01,  4.3854e-01, -4.6624e-01, -5.8753e-02,  7.0586e-02,\n",
       "        -5.2398e-01, -2.0372e-01,  6.0507e-01,  3.5174e-01, -1.6317e-01,\n",
       "         1.7360e-01,  3.2657e-01, -4.6981e-02,  6.2938e-01, -5.2456e-01,\n",
       "         5.7843e-02,  2.3977e-01,  8.2876e-01,  3.2599e-01,  1.0204e-01,\n",
       "         1.5306e-01,  2.7716e-01,  6.9700e-02, -1.8825e-01,  4.7850e-01,\n",
       "         1.8224e-01,  1.1257e-01, -2.4273e-01, -4.0391e-01,  8.0857e-01,\n",
       "         1.3574e-01,  1.8774e-01,  2.3381e-01,  3.3111e-01,  3.3170e-01,\n",
       "        -8.0572e-01,  1.2688e-01,  3.0147e-01,  2.7423e-01,  8.9559e-02,\n",
       "        -4.2699e-01, -6.8097e-02, -2.3789e-03,  2.4720e-02,  6.6309e-02,\n",
       "         1.6751e-01, -4.4892e-01, -3.1938e-02,  1.3843e-01, -4.1223e-01,\n",
       "         2.0473e-01, -4.6821e-01,  1.9463e-02,  1.7132e-01,  2.6854e-02,\n",
       "         4.4851e-01,  2.5116e-02, -2.5346e-01, -4.2672e-01,  2.0244e-01,\n",
       "         6.4817e-02,  4.8698e-01, -1.1511e-01,  4.5536e-01,  3.0381e-01,\n",
       "        -3.0231e-01, -2.2817e-01, -3.4867e-02,  6.8376e-02,  3.8084e-01,\n",
       "        -6.3804e-01, -4.9793e-01,  6.6719e-02, -3.8343e-01,  8.7218e-02,\n",
       "         4.1850e-01, -1.3136e-01, -5.2370e-01,  1.7769e-01, -4.8089e-01,\n",
       "        -2.8214e-01, -3.6037e-01,  2.3933e-01, -6.8329e-01,  1.6390e-02,\n",
       "         3.3606e-01,  4.0516e-01, -4.8257e-01,  6.5687e-01, -2.5093e-01,\n",
       "         3.3531e-01,  6.0436e-01, -4.8631e-01, -4.2262e-01,  3.8172e-01,\n",
       "        -5.6892e-01, -3.5205e-01,  6.5944e-02, -2.0702e-01,  4.1290e-01,\n",
       "        -2.7408e-01,  2.3664e-01,  6.1462e-02, -1.8840e-03,  5.0804e-01,\n",
       "         9.3184e-03, -3.4075e-01,  1.9472e-05,  8.9377e-03,  3.6753e-01,\n",
       "        -7.8663e-02, -2.2789e-01, -4.3862e-02, -6.5197e-02,  2.1685e-01,\n",
       "         2.5838e-01, -1.6728e-01,  3.9059e-01,  1.3423e-01,  2.7705e-01,\n",
       "        -3.4570e-01, -4.1310e-01, -6.1244e-02, -2.2585e-01, -6.5091e-02,\n",
       "         2.2827e-01, -4.9478e-02, -5.2093e-01,  2.8833e-01, -5.9897e-03,\n",
       "        -2.6738e-01, -5.4156e-01, -2.9596e-02,  5.6728e-01, -4.2322e-01,\n",
       "        -3.2112e-01,  2.3251e-01, -5.0625e-01,  9.9522e-02, -2.8304e-01,\n",
       "         7.9547e-03, -7.1061e-01,  6.3622e-02, -2.0892e-01,  9.2263e-02,\n",
       "         1.9543e-01,  1.8830e-01, -5.5699e-01], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.5958e-01, -5.4615e-01, -9.5707e-01,  9.0897e-01,  8.4627e-01,\n",
       "        -2.9216e-01,  9.5695e-01,  4.5178e-01, -9.0960e-01, -1.0000e+00,\n",
       "        -7.2448e-01,  9.8517e-01,  9.8912e-01,  6.7986e-01,  9.7927e-01,\n",
       "        -9.2129e-01, -6.6327e-01, -7.0798e-01,  2.4594e-01, -7.2341e-01,\n",
       "         7.9483e-01,  1.0000e+00, -9.3050e-02,  4.2266e-01,  5.7776e-01,\n",
       "         9.9829e-01, -9.2571e-01,  9.8060e-01,  9.7957e-01,  7.4481e-01,\n",
       "        -8.4806e-01,  2.1180e-01, -9.9392e-01, -3.2780e-01, -9.4844e-01,\n",
       "        -9.9534e-01,  4.3679e-01, -8.2690e-01, -3.3679e-02, -6.8830e-02,\n",
       "        -9.4414e-01,  3.3245e-01,  1.0000e+00,  8.4269e-02,  3.8772e-01,\n",
       "        -3.0375e-01, -1.0000e+00,  3.7404e-01, -9.3661e-01,  9.7277e-01,\n",
       "         9.5867e-01,  9.5275e-01,  2.2533e-01,  5.7421e-01,  5.4911e-01,\n",
       "        -3.1368e-01, -4.3975e-02,  3.4750e-02, -2.8948e-01, -6.9194e-01,\n",
       "        -6.7160e-01,  5.5497e-01, -9.4354e-01, -9.3398e-01,  9.8511e-01,\n",
       "         9.0528e-01, -2.1441e-01, -3.3301e-01, -1.6350e-01, -1.6015e-02,\n",
       "         9.7035e-01,  3.8661e-01,  4.1458e-02, -9.1956e-01,  7.9169e-01,\n",
       "         4.2558e-01, -7.4545e-01,  1.0000e+00, -8.0825e-01, -9.9237e-01,\n",
       "         9.0674e-01,  9.0521e-01,  6.6393e-01, -5.8420e-01,  5.4682e-01,\n",
       "        -1.0000e+00,  7.1329e-01, -3.8643e-02, -9.9513e-01,  2.0340e-01,\n",
       "         7.1972e-01, -3.6655e-01,  3.3399e-01,  6.3355e-01, -7.1416e-01,\n",
       "        -5.7855e-01, -2.6535e-01, -9.4753e-01, -3.5827e-01, -4.3653e-01,\n",
       "         4.0642e-02, -2.6467e-01, -6.5553e-01, -4.6796e-01,  4.2017e-01,\n",
       "        -6.5174e-01, -6.9012e-01,  2.1985e-01,  7.5284e-02,  7.4240e-01,\n",
       "         5.0867e-01, -3.7653e-01,  6.4591e-01, -9.8346e-01,  7.5690e-01,\n",
       "        -4.4012e-01, -9.9446e-01, -6.9032e-01, -9.9500e-01,  7.4352e-01,\n",
       "        -5.7115e-01, -3.1152e-01,  9.8747e-01, -1.7918e-01,  6.3620e-01,\n",
       "        -2.4194e-01, -9.7900e-01, -1.0000e+00, -9.0172e-01, -6.5851e-01,\n",
       "        -4.8829e-01, -4.3089e-01, -9.9053e-01, -9.8352e-01,  6.8020e-01,\n",
       "         9.7393e-01,  2.7397e-01,  9.9998e-01, -3.8190e-01,  9.7523e-01,\n",
       "        -4.1800e-01, -8.2485e-01,  8.4629e-01, -5.6478e-01,  8.5337e-01,\n",
       "         5.9656e-01, -7.9218e-01,  2.1530e-01, -5.4986e-01,  6.1398e-01,\n",
       "        -8.7012e-01, -2.0616e-01, -8.3888e-01, -9.7430e-01, -3.8526e-01,\n",
       "         9.7236e-01, -7.3403e-01, -9.7597e-01, -1.1644e-01, -3.9066e-01,\n",
       "        -5.7758e-01,  8.8114e-01,  8.6428e-01,  4.9535e-01, -5.2551e-01,\n",
       "         5.2816e-01,  3.9386e-01,  7.7606e-01, -9.2116e-01, -4.1869e-01,\n",
       "         6.3788e-01, -4.2085e-01, -9.1960e-01, -9.9065e-01, -4.8766e-01,\n",
       "         6.7451e-01,  9.9512e-01,  8.7446e-01,  3.9144e-01,  9.1738e-01,\n",
       "        -3.5143e-01,  8.9326e-01, -9.8270e-01,  9.9182e-01, -2.4275e-01,\n",
       "         3.4294e-01, -8.2864e-03,  5.5927e-01, -9.2816e-01, -3.2791e-02,\n",
       "         9.2009e-01, -8.3310e-01, -9.2736e-01, -2.5398e-01, -5.2931e-01,\n",
       "        -5.0098e-01, -8.9454e-01,  6.6397e-01, -4.2295e-01, -3.4902e-01,\n",
       "        -5.3927e-02,  9.6675e-01,  9.9503e-01,  8.3961e-01,  5.0602e-01,\n",
       "         8.0525e-01, -9.5189e-01, -6.4821e-01,  1.9254e-01,  3.8860e-01,\n",
       "         1.7349e-01,  9.9750e-01, -7.4020e-01, -2.2951e-01, -9.5440e-01,\n",
       "        -9.9246e-01, -7.0506e-02, -9.6749e-01, -1.6566e-01, -6.9605e-01,\n",
       "         8.3042e-01, -2.4330e-01,  7.7608e-01,  6.0783e-01, -9.9730e-01,\n",
       "        -8.4421e-01,  4.5408e-01, -5.0074e-01,  5.4321e-01, -2.0853e-01,\n",
       "         5.9841e-01,  9.7582e-01, -7.4242e-01,  9.1134e-01,  9.5676e-01,\n",
       "        -9.5332e-01, -8.8915e-01,  9.1387e-01, -4.4544e-01,  9.2665e-01,\n",
       "        -7.7411e-01,  9.9190e-01,  9.7477e-01,  9.2892e-01, -9.6731e-01,\n",
       "        -8.3971e-01, -9.3645e-01, -8.5667e-01, -6.5729e-02,  4.1323e-01,\n",
       "         9.5187e-01,  6.6889e-01,  4.9984e-01,  3.5278e-01, -8.1787e-01,\n",
       "         9.9982e-01, -6.5187e-01, -9.7724e-01, -3.2828e-01, -2.6151e-01,\n",
       "        -9.9266e-01,  9.3869e-01,  4.7628e-01,  3.8986e-01, -4.8022e-01,\n",
       "        -8.5410e-01, -9.8516e-01,  9.4617e-01,  2.2631e-01,  9.9590e-01,\n",
       "        -4.9757e-01, -9.7349e-01, -8.1657e-01, -9.6002e-01, -2.4108e-01,\n",
       "        -2.9450e-01, -5.6502e-01, -1.4657e-01, -9.8157e-01,  6.1308e-01,\n",
       "         6.1881e-01,  6.9481e-01, -9.2317e-01,  9.9992e-01,  1.0000e+00,\n",
       "         9.8519e-01,  9.5097e-01,  9.6940e-01, -9.9999e-01, -7.3806e-01,\n",
       "         1.0000e+00, -9.9884e-01, -1.0000e+00, -9.5853e-01, -7.8942e-01,\n",
       "         4.4853e-01, -1.0000e+00, -2.8244e-01, -1.3621e-01, -9.4771e-01,\n",
       "         8.3885e-01,  9.8761e-01,  9.9917e-01, -1.0000e+00,  8.7891e-01,\n",
       "         9.7379e-01, -7.0062e-01,  9.9414e-01, -5.7500e-01,  9.8509e-01,\n",
       "         5.7304e-01,  4.6392e-01, -3.8685e-01,  5.2649e-01, -9.6531e-01,\n",
       "        -9.3150e-01, -7.4726e-01, -8.7805e-01,  9.9961e-01,  1.3688e-01,\n",
       "        -8.6088e-01, -9.5647e-01,  4.7495e-01, -2.5723e-01, -4.0904e-01,\n",
       "        -9.8504e-01, -2.1554e-01,  8.6024e-01,  9.0420e-01,  2.1498e-01,\n",
       "         4.8909e-01, -7.7118e-01,  3.3913e-01,  4.7480e-02,  3.3904e-01,\n",
       "         7.4643e-01, -9.5175e-01, -5.7691e-01, -2.9600e-01, -2.9186e-02,\n",
       "        -7.9694e-01, -9.8323e-01,  9.8558e-01, -6.1448e-01,  9.7461e-01,\n",
       "         1.0000e+00,  2.2889e-01, -9.5632e-01,  7.8864e-01,  4.0643e-01,\n",
       "        -1.3099e-01,  1.0000e+00,  8.7903e-01, -9.8892e-01, -5.8596e-01,\n",
       "         7.3736e-01, -7.5888e-01, -7.8095e-01,  9.9986e-01, -3.0567e-01,\n",
       "        -8.4219e-01, -7.7469e-01,  9.9055e-01, -9.9205e-01,  9.9885e-01,\n",
       "        -9.6170e-01, -9.8621e-01,  9.8717e-01,  9.7265e-01, -8.8881e-01,\n",
       "        -6.8389e-01,  3.5685e-01, -7.4994e-01,  2.8314e-01, -9.8448e-01,\n",
       "         8.6242e-01,  7.3660e-01, -1.1780e-01,  9.2063e-01, -9.3611e-01,\n",
       "        -6.2041e-01,  2.7217e-01, -8.5755e-01, -3.6993e-01,  9.7917e-01,\n",
       "         6.2808e-01, -2.3489e-01,  1.0577e-01, -4.2891e-01, -2.9914e-01,\n",
       "        -9.9265e-01,  5.0953e-01,  1.0000e+00, -3.8825e-01,  8.4227e-01,\n",
       "        -5.6388e-01, -1.1634e-01, -1.4661e-02,  6.2036e-01,  7.2837e-01,\n",
       "        -3.8430e-01, -9.0520e-01,  8.6466e-01, -9.9497e-01, -9.9170e-01,\n",
       "         8.7534e-01,  2.8752e-01, -4.1031e-01,  1.0000e+00,  6.2460e-01,\n",
       "         1.6925e-01,  6.3732e-01,  9.9883e-01,  6.4139e-02,  7.7103e-01,\n",
       "         9.6069e-01,  9.9120e-01, -3.7327e-01,  6.3081e-01,  9.2961e-01,\n",
       "        -9.6818e-01, -4.0577e-01, -7.1161e-01,  9.5357e-03, -9.2912e-01,\n",
       "        -7.3325e-02, -9.8210e-01,  9.8676e-01,  9.9092e-01,  4.8102e-01,\n",
       "         3.4323e-01,  6.6265e-01,  1.0000e+00, -4.8786e-01,  7.3221e-01,\n",
       "        -6.4999e-01,  9.5004e-01, -9.9995e-01, -9.5911e-01, -5.4015e-01,\n",
       "        -9.1941e-02, -9.2627e-01, -3.9604e-01,  4.7143e-01, -9.8221e-01,\n",
       "         9.0223e-01,  8.1918e-01, -9.9883e-01, -9.9619e-01, -3.5979e-01,\n",
       "         9.5733e-01,  2.3355e-01, -9.9614e-01, -8.5238e-01, -7.0482e-01,\n",
       "         8.1440e-01, -3.7308e-01, -9.7257e-01, -3.5152e-01, -4.1985e-01,\n",
       "         6.5614e-01, -3.2374e-01,  6.5468e-01,  9.3607e-01,  6.4188e-01,\n",
       "        -6.7242e-01, -5.2974e-01, -1.7685e-01, -8.8555e-01,  9.4034e-01,\n",
       "        -9.1540e-01, -9.7426e-01, -3.3156e-01,  1.0000e+00, -6.7283e-01,\n",
       "         9.3071e-01,  8.2869e-01,  8.9243e-01, -1.5310e-01,  2.6069e-01,\n",
       "         9.6392e-01,  3.3215e-01, -8.3703e-01, -9.7157e-01, -8.2858e-01,\n",
       "        -4.7869e-01,  8.0231e-01,  5.3234e-01,  8.0960e-01,  8.6705e-01,\n",
       "         8.5311e-01,  2.0890e-01, -1.2234e-01,  7.9810e-02,  9.9998e-01,\n",
       "        -2.3278e-01, -2.3859e-01, -6.3596e-01, -4.3986e-02, -5.7399e-01,\n",
       "        -5.8051e-01,  1.0000e+00,  3.7883e-01,  5.9675e-01, -9.9440e-01,\n",
       "        -9.7138e-01, -9.6501e-01,  1.0000e+00,  9.2535e-01, -8.7121e-01,\n",
       "         8.2452e-01,  7.4730e-01, -2.1023e-01,  9.0207e-01, -2.9742e-01,\n",
       "        -4.1233e-01,  3.7757e-01,  2.1532e-01,  9.7827e-01, -6.8481e-01,\n",
       "        -9.8766e-01, -7.5888e-01,  6.2681e-01, -9.8573e-01,  9.9999e-01,\n",
       "        -7.1945e-01, -3.9375e-01, -4.3169e-01, -4.3980e-01,  5.1104e-01,\n",
       "        -1.0141e-01, -9.9373e-01, -2.3833e-01,  4.0978e-01,  9.8281e-01,\n",
       "         3.9203e-01, -6.8026e-01, -9.6394e-01,  9.3201e-01,  9.3181e-01,\n",
       "        -9.7999e-01, -9.7448e-01,  9.7984e-01, -9.9545e-01,  7.6478e-01,\n",
       "         1.0000e+00,  3.7255e-01,  4.4474e-01,  3.1073e-01, -6.6140e-01,\n",
       "         4.5333e-01, -3.8528e-01,  8.4906e-01, -9.8810e-01, -4.1741e-01,\n",
       "        -2.8310e-01,  3.6524e-01, -8.4882e-02, -1.5151e-01,  8.4286e-01,\n",
       "         2.5819e-01, -6.5397e-01, -7.6566e-01, -1.5951e-01,  5.5293e-01,\n",
       "         9.2848e-01, -3.1159e-01, -2.7899e-01,  1.1757e-01, -2.0307e-01,\n",
       "        -9.7465e-01, -3.3467e-01, -6.2314e-01, -1.0000e+00,  8.7562e-01,\n",
       "        -1.0000e+00,  6.5728e-01,  4.6709e-01, -2.9505e-01,  9.2268e-01,\n",
       "         5.4866e-01,  7.4234e-01, -8.9428e-01, -9.3962e-01,  3.0641e-01,\n",
       "         8.4488e-01, -2.8753e-01, -8.1319e-01, -7.9154e-01,  4.5888e-01,\n",
       "        -1.3713e-01,  1.8197e-01, -6.0891e-01,  7.9713e-01, -3.4047e-01,\n",
       "         1.0000e+00,  3.0996e-01, -8.6587e-01, -9.9504e-01,  1.8367e-01,\n",
       "        -2.5693e-01,  1.0000e+00, -9.7006e-01, -9.8042e-01,  5.5902e-01,\n",
       "        -8.9941e-01, -9.1073e-01,  5.0530e-01,  1.3198e-01, -8.9611e-01,\n",
       "        -9.8772e-01,  9.8688e-01,  9.6294e-01, -6.9173e-01,  6.2558e-01,\n",
       "        -4.5532e-01, -7.4046e-01,  1.2443e-01,  9.4397e-01,  9.9296e-01,\n",
       "         5.8999e-01,  9.7398e-01,  4.3129e-01, -1.8786e-01,  9.8874e-01,\n",
       "         2.5122e-01,  6.2174e-01,  1.0236e-01,  1.0000e+00,  3.9688e-01,\n",
       "        -9.3600e-01,  6.1009e-02, -9.9095e-01, -3.0345e-01, -9.7863e-01,\n",
       "         4.4081e-01,  2.7751e-01,  9.5581e-01, -2.6770e-01,  9.8368e-01,\n",
       "        -9.5020e-01,  6.5805e-02, -8.9791e-01, -5.8088e-01,  5.1592e-01,\n",
       "        -9.6338e-01, -9.9149e-01, -9.9308e-01,  7.0280e-01, -4.7827e-01,\n",
       "        -9.1018e-02,  2.0499e-01,  2.5813e-01,  5.7039e-01,  6.3572e-01,\n",
       "        -1.0000e+00,  9.7348e-01,  5.7897e-01,  9.7641e-01,  9.8275e-01,\n",
       "         8.3877e-01,  6.2682e-01,  3.6395e-01, -9.9447e-01, -9.9763e-01,\n",
       "        -4.4087e-01, -2.1941e-01,  7.9824e-01,  7.0080e-01,  9.5150e-01,\n",
       "         6.0434e-01, -5.7511e-01, -3.4110e-01, -4.8702e-01, -5.8464e-01,\n",
       "        -9.9707e-01,  5.5351e-01, -7.3683e-01, -9.9189e-01,  9.7489e-01,\n",
       "        -2.4466e-01, -1.2696e-01, -1.5701e-01, -9.0322e-01,  9.8437e-01,\n",
       "         8.6298e-01,  4.8629e-01,  4.3523e-04,  5.1317e-01,  9.4537e-01,\n",
       "         9.7479e-01,  9.8928e-01, -8.8898e-01,  9.2000e-01, -8.5905e-01,\n",
       "         6.4765e-01,  7.3590e-01, -9.6841e-01,  9.2666e-02,  6.8424e-01,\n",
       "        -5.9993e-01,  3.7908e-01, -2.9152e-01, -9.9392e-01,  4.9249e-01,\n",
       "        -4.6171e-01,  7.5058e-01, -5.9175e-01,  6.8797e-03, -4.6865e-01,\n",
       "        -1.6042e-01, -6.5270e-01, -9.0308e-01,  7.1339e-01,  6.9976e-01,\n",
       "         9.5790e-01,  8.8810e-01, -6.0070e-02, -8.0864e-01, -2.3900e-01,\n",
       "        -8.9436e-01, -9.3583e-01,  9.7713e-01, -1.4802e-01, -4.6554e-01,\n",
       "         7.5722e-01,  6.7462e-03,  7.8110e-01,  3.3884e-01, -4.9891e-01,\n",
       "        -4.0055e-01, -8.1258e-01,  9.2492e-01, -6.4619e-01, -6.6872e-01,\n",
       "        -6.8006e-01,  7.6627e-01,  4.2903e-01,  1.0000e+00, -8.9491e-01,\n",
       "        -9.6944e-01, -5.5475e-01, -5.9617e-01,  5.5585e-01, -7.4260e-01,\n",
       "        -1.0000e+00,  5.0934e-01, -6.1901e-01,  7.2725e-01, -8.6563e-01,\n",
       "         9.4413e-01, -8.9636e-01, -9.9350e-01, -3.2186e-01,  5.0956e-01,\n",
       "         8.6238e-01, -5.4705e-01, -8.4899e-01,  6.7073e-01, -5.3554e-01,\n",
       "         9.9653e-01,  9.2667e-01, -8.1040e-01,  6.3296e-02,  7.0009e-01,\n",
       "        -8.2898e-01, -7.9104e-01,  9.6460e-01], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                 \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 10 at dim 1 (got 5)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9aa3ac26165c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2136\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2137\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2138\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2139\u001b[0m             )\n\u001b[1;32m   2140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2321\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2323\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2324\u001b[0m         )\n\u001b[1;32m   2325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         )\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    644\u001b[0m         )\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    609\u001b[0m                     )\n\u001b[1;32m    610\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 611\u001b[0;31m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m                     \u001b[0;34m\"with 'padding=True' 'truncation=True' to have batched tensors with the same length.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text_batch, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank. [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "print (marked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'after', 'stealing', 'money', 'from', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5000:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 101)\n",
      "('after', 2044)\n",
      "('stealing', 11065)\n",
      "('money', 2769)\n",
      "('from', 2013)\n",
      "('the', 1996)\n",
      "('bank', 2924)\n",
      "('vault', 11632)\n",
      "(',', 1010)\n",
      "('the', 1996)\n",
      "('bank', 2924)\n",
      "('robber', 27307)\n",
      "('was', 2001)\n",
      "('seen', 2464)\n",
      "('fishing', 5645)\n",
      "('on', 2006)\n",
      "('the', 1996)\n",
      "('mississippi', 5900)\n",
      "('river', 2314)\n",
      "('bank', 2924)\n",
      "('.', 1012)\n",
      "('[SEP]', 102)\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print (tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 8.70 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "831 ms ± 453 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Number of batches: 1\n",
      "Number of tokens: 22\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 5\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec.cpu(), bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 22\n",
      "Number of layers per token: 12\n"
     ]
    }
   ],
   "source": [
    "# Convert the hidden state embeddings into single token vectors\n",
    "\n",
    "# Holds the list of 12 layer embeddings for each token\n",
    "# Will have the shape: [# tokens, # layers, # features]\n",
    "token_embeddings = [] \n",
    "\n",
    "# For each token in the sentence...\n",
    "for token_i in range(len(tokenized_text)):\n",
    "  \n",
    "    # Holds 12 layers of hidden states for each token \n",
    "    hidden_layers = [] \n",
    "  \n",
    "    # For each of the 12 layers...\n",
    "    for layer_i in range(len(encoded_layers)):\n",
    "    \n",
    "        # Lookup the vector for `token_i` in `layer_i`\n",
    "        vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "    \n",
    "        hidden_layers.append(vec)\n",
    "    \n",
    "    token_embeddings.append(hidden_layers)\n",
    "\n",
    "# Sanity check the dimensions:\n",
    "print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
    "print (\"Number of layers per token:\", len(token_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 3072\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), 0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(torch.stack(token)[-4:], 0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = torch.mean(encoded_layers[11], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organised embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_embed_sentence(sent, model, tokenizer):\n",
    "    '''returns a list (with length same as number of layers) of tensors each of shape \n",
    "    [batch_size, len of tokenised sent (+2), hidden state shape]'''\n",
    "    if isinstance(sent, str) is False:\n",
    "        print(sent)\n",
    "    # add special start and end tokens\n",
    "    marked_text = \"[CLS] \" + sent + \" [SEP]\"\n",
    "    \n",
    "    # tokenise text\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    \n",
    "    # turn tokens into BERT number ids\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    # generate token used to indicate that all words are from the same sent\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "    \n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        print(tokens_tensor)\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "        print(\"encoded\", len(encoded_layers), encoded_layers[0].shape)\n",
    "        print(_)\n",
    "        print(encoded_layers)\n",
    "    return encoded_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence_mean_layer11(sent, model, tokenizer):\n",
    "    encoded_layers = raw_embed_sentence(sent, model, tokenizer)\n",
    "    return torch.mean(encoded_layers, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence_mean_layer11(sent, model, tokenizer):\n",
    "    encoded_layers = raw_embed_sentence(sent, model, tokenizer)\n",
    "    return torch.mean(encoded_layers[11], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 12476,  2001,  2023,  3185,   102]])\n",
      "encoded 12 torch.Size([1, 6, 768])\n",
      "tensor([[-0.6578, -0.2301,  0.5664,  0.2902, -0.4416, -0.1039,  0.5521,  0.1852,\n",
      "          0.2588, -0.9955,  0.2026,  0.1015,  0.9470, -0.5158,  0.6793, -0.0984,\n",
      "         -0.0680, -0.3631,  0.3193, -0.1651,  0.2337,  0.3335,  0.5986,  0.1670,\n",
      "          0.2544, -0.1337, -0.5863,  0.7485,  0.8689,  0.5938, -0.3730,  0.2113,\n",
      "         -0.9520, -0.1433,  0.4930, -0.9439,  0.0173, -0.5513,  0.1341,  0.0446,\n",
      "         -0.6650,  0.2010,  0.9348, -0.4095, -0.2550, -0.2620, -0.9760,  0.0578,\n",
      "         -0.6820, -0.5918, -0.5665, -0.6967,  0.0738,  0.1732,  0.2821,  0.4861,\n",
      "         -0.1203,  0.1568, -0.0574, -0.4140, -0.4608,  0.1571,  0.2286, -0.7045,\n",
      "         -0.3777, -0.5752, -0.0884, -0.0336,  0.0247, -0.0897,  0.6677,  0.1309,\n",
      "          0.4357, -0.5596, -0.5488,  0.1474, -0.2535,  0.9978, -0.0817, -0.9218,\n",
      "         -0.2290, -0.2923,  0.1060,  0.6289, -0.6815, -0.9924,  0.1253, -0.0567,\n",
      "         -0.9580,  0.0638, -0.0477, -0.1453, -0.5094,  0.1572, -0.0995,  0.1386,\n",
      "         -0.0318,  0.4459, -0.0828,  0.0618, -0.0988, -0.0637,  0.1662, -0.1966,\n",
      "          0.0047, -0.2241, -0.0973,  0.0240, -0.3272,  0.3973,  0.1735, -0.1328,\n",
      "          0.0300, -0.8622,  0.4246, -0.0373, -0.9380, -0.2114, -0.9618,  0.3238,\n",
      "          0.1317, -0.1006,  0.8596,  0.5914,  0.1578,  0.1116,  0.5387, -0.9987,\n",
      "          0.0522,  0.2715,  0.2711, -0.0232, -0.9310, -0.8715,  0.4650,  0.8142,\n",
      "          0.0502,  0.9410, -0.0576,  0.8309,  0.3197, -0.1106, -0.4428, -0.2375,\n",
      "         -0.0723, -0.0085, -0.3456,  0.1579,  0.3445, -0.2118,  0.1263, -0.1140,\n",
      "          0.5396, -0.8081, -0.2737,  0.7536,  0.3610,  0.5809,  0.5973, -0.0079,\n",
      "         -0.0757,  0.6020, -0.1027,  0.1961, -0.1778,  0.2710, -0.5017,  0.2358,\n",
      "         -0.5326,  0.3401,  0.2376, -0.0949,  0.5091, -0.9268, -0.1704,  0.1918,\n",
      "          0.9393,  0.5717,  0.0757, -0.2380, -0.0023,  0.0949, -0.8768,  0.9241,\n",
      "         -0.0662,  0.1796,  0.7479,  0.0047, -0.5432, -0.4649,  0.5351, -0.0027,\n",
      "         -0.6667,  0.1829, -0.3791, -0.2523,  0.4975,  0.2096, -0.1204, -0.1752,\n",
      "          0.1374,  0.7124,  0.6442,  0.3530, -0.4428,  0.2179, -0.7478, -0.2203,\n",
      "          0.0518,  0.0934,  0.0789,  0.9760,  0.2197,  0.0170, -0.6776, -0.9536,\n",
      "         -0.0388, -0.6914,  0.0089, -0.4429, -0.0152,  0.6592, -0.3418,  0.1645,\n",
      "         -0.8093, -0.5402,  0.0558, -0.1403,  0.2570, -0.0743,  0.5611, -0.3924,\n",
      "         -0.3898,  0.3385,  0.7971,  0.5549, -0.4331,  0.3960, -0.1393,  0.7470,\n",
      "         -0.2866,  0.7982, -0.2533,  0.3866, -0.7229,  0.2139, -0.6044,  0.2855,\n",
      "         -0.0051, -0.7697, -0.3550,  0.1275,  0.1374,  0.8501, -0.2573,  0.9271,\n",
      "         -0.8099, -0.8645,  0.1252, -0.0419, -0.9409, -0.4755,  0.1380, -0.7004,\n",
      "         -0.1860,  0.0472, -0.8556,  0.5038,  0.0324,  0.8176,  0.3702, -0.6853,\n",
      "         -0.1414, -0.7677, -0.2502,  0.0228,  0.6122, -0.1460, -0.8000,  0.3381,\n",
      "          0.1647,  0.1648,  0.6088,  0.9476,  0.9314,  0.9291,  0.6567,  0.6136,\n",
      "         -0.1185, -0.2946,  0.9972, -0.0447, -0.9837, -0.7573, -0.2533, -0.0444,\n",
      "         -0.9982, -0.1157,  0.1416, -0.7832, -0.5992,  0.9365,  0.8145, -0.9962,\n",
      "          0.5186,  0.7672, -0.2649, -0.0930,  0.1624,  0.9068,  0.2491,  0.2464,\n",
      "         -0.0706,  0.1294,  0.2125, -0.5719,  0.4369,  0.4076, -0.3659, -0.0213,\n",
      "         -0.4165, -0.7879, -0.3511, -0.0807, -0.2903, -0.8503,  0.0186, -0.3490,\n",
      "          0.3254,  0.0066,  0.0440, -0.4331,  0.0166, -0.7195,  0.0472,  0.2947,\n",
      "         -0.8576,  0.0245,  0.3477, -0.2484,  0.3818, -0.8706,  0.8773, -0.2506,\n",
      "         -0.4610,  0.9973, -0.1429, -0.5661,  0.0433, -0.1533,  0.2735,  0.9962,\n",
      "          0.1048, -0.9312, -0.2807, -0.0041, -0.2421, -0.1276,  0.9699, -0.0792,\n",
      "          0.4278,  0.5545,  0.9367, -0.9644, -0.3833, -0.7681, -0.8836,  0.8894,\n",
      "          0.8117, -0.1629, -0.0975,  0.0705,  0.4938,  0.0797, -0.8179,  0.4142,\n",
      "          0.1134, -0.0777,  0.6567, -0.5188, -0.1483,  0.1876,  0.0562,  0.2549,\n",
      "         -0.5364,  0.2566, -0.0356,  0.0607, -0.0458, -0.0290, -0.9107, -0.2349,\n",
      "          0.9941,  0.0733, -0.7392, -0.1043, -0.0086, -0.3535,  0.1835,  0.1909,\n",
      "         -0.0845, -0.4483, -0.3905, -0.6597, -0.9542,  0.3297,  0.0596, -0.2074,\n",
      "          0.9138,  0.2696, -0.0079,  0.0284, -0.2718, -0.1109,  0.3061, -0.5985,\n",
      "          0.8896, -0.1617,  0.1572,  0.4569,  0.4734, -0.2135, -0.3434, -0.0150,\n",
      "         -0.8278,  0.0560, -0.8499,  0.8424, -0.5871,  0.1414,  0.1064, -0.4642,\n",
      "          0.9951,  0.1776,  0.1606, -0.0438,  0.5289, -0.3032, -0.3449, -0.2918,\n",
      "          0.1069,  0.4799, -0.0222,  0.0872, -0.9068, -0.5024, -0.3513, -0.8288,\n",
      "         -0.9616,  0.3621,  0.3102,  0.0147,  0.3018, -0.3613, -0.3523,  0.0175,\n",
      "          0.0743, -0.8541,  0.5351, -0.1349,  0.3177, -0.1074,  0.2194, -0.5947,\n",
      "          0.8511,  0.7247,  0.2540,  0.0305, -0.5668,  0.3882, -0.4157,  0.6150,\n",
      "         -0.0623,  0.9978, -0.2626, -0.3926,  0.3694,  0.4835,  0.0747,  0.0457,\n",
      "         -0.4312,  0.2034,  0.5510,  0.6721, -0.3799, -0.1288,  0.3170, -0.5966,\n",
      "         -0.6800,  0.4276, -0.1241,  0.1177,  0.0437, -0.0777,  0.9694, -0.0970,\n",
      "          0.0235, -0.3677,  0.1316, -0.2400, -0.1313,  0.9877,  0.1661, -0.4179,\n",
      "         -0.9658,  0.5612, -0.5485,  0.8430,  0.6530, -0.3504,  0.2441,  0.1504,\n",
      "         -0.0982,  0.3633, -0.0611, -0.1620,  0.1740,  0.0622,  0.8870, -0.2537,\n",
      "         -0.9122, -0.3097,  0.0855, -0.8438,  0.2695, -0.1996, -0.0913, -0.1030,\n",
      "          0.4632,  0.1207, -0.1835, -0.9244, -0.0245, -0.1031,  0.8870,  0.0346,\n",
      "         -0.2693, -0.7669, -0.5895, -0.2443,  0.5231, -0.8212,  0.8952, -0.8955,\n",
      "          0.2072,  0.9892,  0.1723, -0.7205, -0.0151, -0.1673,  0.0041,  0.0418,\n",
      "          0.1588, -0.8348, -0.1864, -0.0456,  0.2498, -0.0459,  0.1117,  0.3165,\n",
      "          0.1517, -0.1114, -0.3842,  0.1348,  0.2117,  0.4108, -0.2223, -0.0590,\n",
      "          0.1209, -0.0538, -0.6029, -0.1769, -0.0551, -0.6000,  0.3657, -0.9976,\n",
      "         -0.5011, -0.6508, -0.1745,  0.6557,  0.2247, -0.4171, -0.3101,  0.5779,\n",
      "          0.8150,  0.3674, -0.0342,  0.4310, -0.3227, -0.0282, -0.0363,  0.2735,\n",
      "          0.4220,  0.5423, -0.0132,  0.9988,  0.0040, -0.0146, -0.8151,  0.1280,\n",
      "         -0.1132,  0.9528, -0.6087, -0.8898,  0.0794, -0.1852, -0.5896,  0.0722,\n",
      "          0.0505, -0.4995,  0.1335,  0.6435,  0.6019, -0.2546,  0.1217, -0.1897,\n",
      "         -0.1353, -0.0380, -0.6609,  0.9622, -0.1797,  0.6096,  0.1305,  0.0514,\n",
      "          0.8511,  0.2352,  0.0507, -0.0294,  0.9887,  0.0540, -0.8012,  0.3013,\n",
      "         -0.9211, -0.1468, -0.8601,  0.1680, -0.1131,  0.7254, -0.1423,  0.7999,\n",
      "          0.6535,  0.0286,  0.2554,  0.7692,  0.2411, -0.7114, -0.9514, -0.9613,\n",
      "          0.1101, -0.2754,  0.0992,  0.2357,  0.1362,  0.0846,  0.2431, -0.9871,\n",
      "          0.8163,  0.2127, -0.5672,  0.9055,  0.0045,  0.1667,  0.1046, -0.9521,\n",
      "         -0.6768, -0.2061, -0.1582,  0.3092,  0.2673,  0.6909,  0.1740, -0.3214,\n",
      "          0.0647,  0.5483, -0.7666, -0.9768,  0.2478,  0.3679, -0.7869,  0.8697,\n",
      "         -0.4000, -0.0462,  0.5595,  0.4290,  0.6271,  0.3152,  0.2390,  0.0355,\n",
      "          0.3173,  0.7534,  0.7263,  0.9524,  0.2802,  0.3345,  0.5524,  0.1281,\n",
      "          0.6098, -0.8379,  0.0609, -0.2553,  0.1422,  0.1375, -0.0711, -0.7531,\n",
      "         -0.0070, -0.2719,  0.2704, -0.1253,  0.1698, -0.2659, -0.0130, -0.2901,\n",
      "         -0.3237,  0.3634, -0.0496,  0.7999, -0.1318, -0.0218, -0.1710, -0.0136,\n",
      "          0.6320, -0.8207,  0.5364,  0.1030,  0.6297, -0.6733, -0.1417,  0.3497,\n",
      "         -0.3828, -0.2161, -0.2131, -0.5651,  0.4720,  0.0021, -0.1750, -0.2436,\n",
      "          0.2622,  0.1276,  0.3583,  0.3820,  0.3148,  0.0180, -0.0760,  0.0723,\n",
      "          0.0710, -0.9868,  0.0795,  0.1532, -0.2485,  0.2694, -0.4104,  0.2672,\n",
      "         -0.8029,  0.0333, -0.0893, -0.3634, -0.4277, -0.0961,  0.1651,  0.5799,\n",
      "         -0.3838,  0.7080,  0.6345,  0.5556,  0.2409,  0.5860, -0.4525,  0.7231]])\n",
      "[tensor([[[ 0.1591, -0.0296, -0.0897,  ...,  0.0354,  0.1223,  0.0365],\n",
      "         [ 1.0501, -0.2485,  0.0388,  ...,  0.5496,  0.1053, -0.1128],\n",
      "         [-0.5383, -0.4206,  0.2913,  ..., -0.0108,  0.3061, -0.0470],\n",
      "         [-0.1014,  0.1602,  0.1623,  ...,  0.0152,  0.5371,  0.0736],\n",
      "         [ 0.2799,  0.0814, -0.0943,  ...,  0.4727,  0.4635, -0.8647],\n",
      "         [ 0.0124, -0.0115, -0.0753,  ..., -0.1550,  0.9868,  0.0868]]]), tensor([[[ 0.0468, -0.1631, -0.1563,  ...,  0.0974,  0.1379,  0.0671],\n",
      "         [ 1.2329, -0.0436,  0.3858,  ...,  0.6710, -0.4671, -0.3081],\n",
      "         [-0.3665,  0.0886,  0.6237,  ...,  0.0324,  0.3526, -0.3659],\n",
      "         [ 0.2830,  0.1297,  0.7506,  ...,  0.2183,  0.7081,  0.0065],\n",
      "         [ 0.6662,  0.4680,  0.0718,  ...,  0.2300,  0.4960, -0.9228],\n",
      "         [-0.1272,  0.1188,  0.1809,  ...,  0.1132,  0.7992, -0.0658]]]), tensor([[[ 0.0878, -0.1616, -0.0360,  ...,  0.1583,  0.1160,  0.2078],\n",
      "         [ 1.3299, -0.4918,  0.3258,  ...,  0.4297, -0.2571, -0.2916],\n",
      "         [-0.5831, -0.1930,  0.7648,  ..., -0.0272,  0.3650, -0.3287],\n",
      "         [ 0.1596,  0.0923,  1.3082,  ...,  0.0326,  0.2449,  0.1399],\n",
      "         [ 0.5378,  0.3628, -0.0253,  ...,  0.0560,  0.2500, -1.2794],\n",
      "         [-0.0298, -0.0409,  0.1292,  ...,  0.0839,  0.1301, -0.0242]]]), tensor([[[-0.0019, -0.7163, -0.6923,  ...,  0.4039,  0.0022,  0.4771],\n",
      "         [ 1.7418, -0.4569, -0.3447,  ...,  0.0545, -0.9553, -0.5130],\n",
      "         [-0.0900, -0.2933,  0.4363,  ..., -0.1554,  0.0570,  0.3687],\n",
      "         [ 0.0643, -0.3943,  0.8869,  ..., -0.1591,  0.5262, -0.0622],\n",
      "         [ 0.7296,  0.4106, -0.4789,  ..., -0.0896,  0.0138, -0.8391],\n",
      "         [-0.0237, -0.0195,  0.0077,  ...,  0.0222,  0.0491, -0.0193]]]), tensor([[[-0.2550, -0.6468, -0.4174,  ...,  0.3491,  0.3043,  0.4238],\n",
      "         [ 1.4987, -0.3436, -0.6285,  ..., -0.1106, -0.7080, -0.5832],\n",
      "         [ 0.0705, -0.5897, -0.1819,  ...,  0.0274,  0.9517,  0.3387],\n",
      "         [-0.0114, -0.4579,  1.2139,  ...,  0.5159,  0.9696,  0.1498],\n",
      "         [ 0.7708,  0.5723, -0.1224,  ...,  0.4397, -0.4335, -1.0599],\n",
      "         [-0.0162, -0.0126, -0.0028,  ...,  0.0387, -0.0072, -0.0317]]]), tensor([[[-0.0802, -0.7771, -0.1087,  ...,  0.3638,  0.7889,  0.3894],\n",
      "         [ 1.3843, -0.7390, -0.6375,  ...,  0.3730, -0.5714, -0.8314],\n",
      "         [ 0.0506, -0.8616, -0.0580,  ..., -0.3866,  1.3637, -0.0531],\n",
      "         [-0.4462, -0.5453,  0.7143,  ...,  0.2772,  0.7912,  0.5447],\n",
      "         [ 0.5861,  0.9112, -0.3571,  ...,  0.2204, -0.4507, -1.0515],\n",
      "         [ 0.0073, -0.0072, -0.0169,  ...,  0.0225, -0.0254, -0.0328]]]), tensor([[[ 0.2715, -0.8379, -0.5900,  ...,  0.3629,  0.7782,  0.3898],\n",
      "         [ 1.2291, -0.7767, -0.7232,  ...,  0.3854, -0.3917, -1.1675],\n",
      "         [ 0.1496, -1.0971, -0.2662,  ..., -0.5427,  1.1111, -0.1302],\n",
      "         [-0.4281, -0.8323, -0.1961,  ...,  0.1622,  0.6624,  0.2456],\n",
      "         [ 0.4098,  0.5604, -0.5530,  ...,  0.6573, -0.4943, -1.2151],\n",
      "         [ 0.0192, -0.0055, -0.0249,  ..., -0.0063,  0.0092, -0.0588]]]), tensor([[[ 4.6518e-01, -4.6772e-01, -5.6468e-01,  ..., -1.6539e-01,\n",
      "           3.5218e-01,  3.4294e-02],\n",
      "         [ 1.0517e+00, -4.9909e-01, -3.4459e-01,  ..., -1.9034e-04,\n",
      "          -1.2941e-01, -1.1945e+00],\n",
      "         [ 7.5601e-01, -1.6624e+00, -5.3670e-01,  ..., -4.0133e-01,\n",
      "           5.2742e-01,  4.1754e-02],\n",
      "         [-1.9061e-01, -1.1604e+00,  8.9967e-02,  ..., -4.0174e-03,\n",
      "           5.6966e-01,  5.3320e-02],\n",
      "         [ 7.1906e-01,  8.0169e-01, -8.1950e-01,  ...,  5.2672e-01,\n",
      "          -3.6056e-01, -1.2244e+00],\n",
      "         [ 3.6960e-02, -3.1975e-03,  2.0749e-02,  ..., -1.5682e-02,\n",
      "          -5.6272e-02, -5.6895e-02]]]), tensor([[[ 0.3255, -0.1506, -0.3967,  ..., -0.2835,  0.0179,  0.0476],\n",
      "         [ 0.9571, -0.4312, -0.2227,  ..., -0.0254,  0.2426, -1.0044],\n",
      "         [ 0.4127, -1.0548, -0.0254,  ..., -0.0854, -0.1597,  0.1767],\n",
      "         [-0.6803, -0.8060,  0.4191,  ..., -0.3232,  0.2186, -0.1105],\n",
      "         [ 0.4656,  0.6073, -1.0422,  ...,  0.0331, -0.3666, -1.3461],\n",
      "         [-0.0039,  0.0215,  0.0060,  ..., -0.0623, -0.1033, -0.0027]]]), tensor([[[ 0.2130, -0.5652, -0.0999,  ..., -0.6271, -0.3321,  0.0958],\n",
      "         [ 0.3863, -0.5949, -0.1485,  ..., -0.0834, -0.0554, -1.1630],\n",
      "         [ 0.0181, -0.9524,  0.1770,  ..., -0.2990, -0.6519,  0.4266],\n",
      "         [-0.9133, -0.8877,  0.3787,  ..., -1.0029,  0.1269,  0.0075],\n",
      "         [ 0.2832, -0.0861, -0.8199,  ..., -0.0332, -0.4689, -1.3520],\n",
      "         [-0.0094, -0.0202, -0.0721,  ...,  0.0285, -0.0170, -0.0178]]]), tensor([[[ 0.2364, -0.2932,  0.1174,  ..., -0.5552, -0.4675,  0.0017],\n",
      "         [-0.1123, -0.3445, -0.0038,  ..., -0.5152, -0.1315, -0.9003],\n",
      "         [-0.1676, -1.3902,  0.8225,  ..., -0.4369, -0.1267,  0.5773],\n",
      "         [-1.1761, -0.6933,  0.6613,  ..., -1.3739,  0.1011,  0.6725],\n",
      "         [ 0.3017, -0.2775, -0.2635,  ..., -0.4624, -0.2994, -0.4570],\n",
      "         [ 0.0264,  0.0149, -0.0196,  ...,  0.0131, -0.0138,  0.0158]]]), tensor([[[ 1.5708e-01,  5.8881e-02,  2.4838e-01,  ..., -4.0060e-01,\n",
      "          -7.8383e-04,  9.1403e-02],\n",
      "         [-1.3791e-01, -1.2978e-01, -1.0722e-01,  ..., -3.3510e-01,\n",
      "           3.3484e-01, -4.5329e-01],\n",
      "         [-7.6716e-02, -8.9061e-01,  5.7112e-01,  ...,  2.1575e-02,\n",
      "           2.7492e-01,  4.5951e-01],\n",
      "         [-8.2786e-01, -5.1681e-01,  5.0772e-01,  ..., -1.0407e+00,\n",
      "          -2.0520e-03,  3.6517e-01],\n",
      "         [ 3.1375e-01, -1.1551e-01, -2.9069e-01,  ..., -2.0932e-01,\n",
      "          -2.8874e-01, -4.2532e-01],\n",
      "         [ 7.7283e-01, -1.3175e-01, -2.0073e-01,  ...,  9.1713e-02,\n",
      "          -5.9273e-01, -1.3191e-01]]])]\n"
     ]
    }
   ],
   "source": [
    "# # Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "b = embed_sentence_mean_layer11(\"awesome was this movie\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 12476,  2001,  2023,  3185,   102]])\n",
      "encoded 1 torch.Size([6, 768])\n",
      "tensor([[-8.4071e-01, -2.5620e-01,  2.7098e-01,  5.6198e-01, -1.0199e-01,\n",
      "         -1.3761e-01,  8.1588e-01,  1.6933e-01, -1.6499e-02, -9.9948e-01,\n",
      "          1.7728e-02,  4.5266e-01,  9.5724e-01, -2.8368e-01,  8.6791e-01,\n",
      "         -4.5208e-01, -2.6012e-01, -4.4431e-01,  3.2822e-01, -6.3699e-01,\n",
      "          4.7634e-01,  8.8808e-01,  6.0551e-01,  1.9023e-01,  3.3571e-01,\n",
      "          5.8984e-01, -6.3266e-01,  8.7245e-01,  9.1427e-01,  5.8373e-01,\n",
      "         -5.5130e-01,  1.3810e-01, -9.5917e-01, -1.5259e-01,  2.9504e-01,\n",
      "         -9.6428e-01,  1.0432e-01, -7.2401e-01,  1.0018e-01,  3.2127e-02,\n",
      "         -8.0474e-01,  1.8206e-01,  9.9082e-01, -3.6956e-01, -2.3794e-01,\n",
      "         -2.7829e-01, -9.9969e-01,  1.4882e-01, -7.6032e-01, -1.8868e-01,\n",
      "         -1.7025e-01, -3.4416e-01,  1.4183e-01,  3.1765e-01,  3.1312e-01,\n",
      "          3.6314e-01, -1.1273e-01,  1.2589e-01, -8.4020e-02, -4.6947e-01,\n",
      "         -4.8408e-01,  1.2776e-01, -3.7328e-02, -8.4061e-01,  4.1840e-02,\n",
      "         -3.0340e-01, -5.7717e-02, -1.5950e-01, -5.8675e-02, -5.3666e-02,\n",
      "          8.0825e-01,  1.7727e-01,  3.7117e-01, -7.0752e-01, -3.2607e-01,\n",
      "          1.8001e-01, -3.3056e-01,  1.0000e+00, -3.3601e-01, -9.4670e-01,\n",
      "         -1.7544e-01, -9.6281e-02,  1.9208e-01,  4.9887e-01, -4.3660e-01,\n",
      "         -9.9997e-01,  2.8957e-01, -1.0503e-02, -9.7356e-01,  1.0780e-01,\n",
      "          2.6072e-01, -1.3030e-01, -4.3862e-01,  2.4199e-01, -2.4496e-01,\n",
      "         -8.6550e-02, -1.6277e-01,  1.3136e-01, -7.9211e-02,  1.3561e-02,\n",
      "         -5.8562e-02, -1.2026e-01,  7.3660e-03, -1.9953e-01,  4.6357e-02,\n",
      "         -2.8870e-01, -3.7910e-01,  1.6581e-01, -1.9997e-01,  5.1694e-01,\n",
      "          1.5962e-01, -1.7290e-01,  2.4240e-01, -9.2471e-01,  5.2703e-01,\n",
      "         -1.1594e-01, -9.5487e-01, -3.0458e-01, -9.7040e-01,  4.6959e-01,\n",
      "          8.6914e-02, -1.0169e-01,  9.2237e-01,  5.4278e-01,  2.0563e-01,\n",
      "          5.9020e-02,  1.9781e-01, -1.0000e+00, -2.9521e-01,  1.0302e-01,\n",
      "          1.7344e-01, -4.3072e-02, -9.4135e-01, -8.9053e-01,  4.8770e-01,\n",
      "          9.1036e-01,  7.2774e-02,  9.8772e-01, -1.2442e-01,  8.6300e-01,\n",
      "          1.0041e-01, -2.3046e-01, -1.5214e-01, -3.4612e-01,  1.8236e-01,\n",
      "          2.4264e-01, -5.7540e-01,  1.4196e-01,  2.3120e-01, -1.0428e-01,\n",
      "         -9.7282e-03, -1.1909e-01,  2.3793e-01, -8.9045e-01, -3.4919e-01,\n",
      "          8.7079e-01,  1.1558e-01,  2.0307e-01,  5.3899e-01, -1.1138e-01,\n",
      "         -2.3631e-01,  7.4156e-01,  1.8603e-01,  2.3358e-01, -9.7702e-02,\n",
      "          3.1324e-01, -1.9881e-01,  3.2937e-01, -6.9242e-01,  1.7509e-01,\n",
      "          3.4071e-01, -1.5003e-01,  3.4168e-01, -9.3886e-01, -2.0653e-01,\n",
      "          3.6837e-01,  9.6047e-01,  6.5799e-01,  1.3528e-01,  9.7842e-02,\n",
      "         -6.5883e-02,  2.9497e-01, -8.9505e-01,  9.4065e-01, -1.5551e-01,\n",
      "          1.4784e-01,  6.6144e-01,  6.9009e-02, -7.4921e-01, -3.9168e-01,\n",
      "          7.6038e-01, -2.3334e-01, -7.5580e-01,  1.2450e-01, -3.6767e-01,\n",
      "         -3.2605e-01,  2.2729e-01,  3.3198e-01, -2.0891e-01, -2.6043e-01,\n",
      "          4.6824e-02,  8.3060e-01,  9.3312e-01,  6.2220e-01, -4.7547e-01,\n",
      "          4.4544e-01, -8.4380e-01, -2.8431e-01,  1.6466e-02,  1.2320e-01,\n",
      "          1.2130e-01,  9.8180e-01,  1.8924e-01, -8.5906e-02, -8.5955e-01,\n",
      "         -9.6377e-01, -2.1741e-02, -8.0826e-01, -2.7240e-02, -5.2623e-01,\n",
      "          1.2472e-01,  6.1559e-01, -2.1135e-04,  2.5593e-01, -9.7534e-01,\n",
      "         -6.9624e-01,  1.8727e-01, -1.4968e-01,  2.6194e-01, -1.1553e-01,\n",
      "          1.6961e-01,  7.4110e-02, -4.1412e-01,  7.6086e-01,  7.9004e-01,\n",
      "          2.2701e-01, -4.9224e-01,  7.4927e-01, -2.0664e-01,  8.3175e-01,\n",
      "         -4.4173e-01,  9.3465e-01,  8.8167e-02,  5.6285e-01, -8.4469e-01,\n",
      "          1.3363e-01, -8.4376e-01,  2.2221e-02,  6.9765e-03, -5.8103e-01,\n",
      "          4.8599e-02,  2.5669e-01,  2.3908e-01,  7.7619e-01, -4.5543e-01,\n",
      "          9.9147e-01, -3.4952e-01, -8.9616e-01,  3.8735e-01, -4.5652e-02,\n",
      "         -9.5863e-01, -9.7077e-02,  2.6149e-01, -6.4046e-01, -2.4173e-01,\n",
      "         -2.6394e-01, -8.8935e-01,  7.9608e-01,  6.9815e-02,  9.6554e-01,\n",
      "          1.9410e-01, -8.5289e-01, -3.3493e-01, -8.0620e-01, -2.8520e-01,\n",
      "         -4.6426e-02,  4.7480e-01, -1.7346e-01, -9.0564e-01,  3.6977e-01,\n",
      "          3.0228e-01,  2.8568e-01,  4.0801e-01,  9.9280e-01,  9.9720e-01,\n",
      "          9.3910e-01,  8.0434e-01,  8.4905e-01, -7.5363e-01, -1.3409e-01,\n",
      "          9.9973e-01, -6.7360e-01, -9.9986e-01, -8.7274e-01, -4.9707e-01,\n",
      "          1.6795e-01, -1.0000e+00, -6.1488e-02,  1.1866e-01, -8.4332e-01,\n",
      "         -3.0300e-01,  9.5132e-01,  9.7292e-01, -9.9999e-01,  7.4800e-01,\n",
      "          8.7587e-01, -3.2551e-01,  4.6095e-01, -5.9039e-03,  9.3133e-01,\n",
      "          3.5376e-01,  2.0215e-01, -1.9340e-01,  2.3340e-01, -9.3769e-02,\n",
      "         -7.3964e-01,  2.9223e-01,  2.0475e-01,  4.0672e-01,  7.0719e-02,\n",
      "         -6.0631e-01, -8.7344e-01, -3.2952e-01, -1.2085e-01, -3.0609e-01,\n",
      "         -9.1261e-01, -6.7448e-03, -1.1837e-01,  5.5122e-01,  8.3285e-02,\n",
      "          6.4896e-02, -6.7177e-01,  1.1008e-01, -6.5075e-01,  3.1143e-01,\n",
      "          3.9051e-01, -8.9636e-01, -4.4864e-01, -3.4738e-02, -3.4900e-01,\n",
      "          2.0457e-01, -8.9629e-01,  9.3466e-01, -3.3010e-01, -3.0710e-02,\n",
      "          1.0000e+00, -3.5573e-01, -7.8586e-01,  2.0782e-01,  3.6284e-02,\n",
      "          1.4990e-01,  9.9999e-01,  3.5392e-01, -9.4304e-01, -2.8852e-01,\n",
      "          1.2505e-01, -3.4589e-01, -2.2970e-01,  9.9108e-01, -1.7585e-01,\n",
      "          2.1705e-01,  4.1305e-01,  9.2660e-01, -9.6806e-01,  2.0274e-01,\n",
      "         -8.2982e-01, -9.2019e-01,  9.2350e-01,  8.7537e-01, -2.7685e-01,\n",
      "         -4.6874e-01,  1.0061e-01,  1.0857e-01,  1.2328e-01, -9.3572e-01,\n",
      "          6.2215e-01,  3.0240e-01, -6.4230e-02,  8.2143e-01, -7.9912e-01,\n",
      "         -2.1978e-01,  3.0055e-01, -1.8376e-01,  1.7083e-01, -1.3722e-01,\n",
      "          3.4513e-01, -9.7875e-02,  9.1437e-02, -1.5148e-01,  2.6326e-01,\n",
      "         -9.5394e-01, -1.0102e-01,  9.9998e-01,  1.9161e-02, -5.2562e-01,\n",
      "         -2.4244e-01, -1.0376e-02, -3.3938e-01,  2.5740e-01,  3.5436e-01,\n",
      "         -1.9868e-01, -7.1226e-01, -1.1096e-01, -8.9787e-01, -9.5700e-01,\n",
      "          6.7892e-01,  1.6055e-01, -2.3008e-01,  9.9651e-01,  3.0645e-01,\n",
      "          8.1616e-02,  5.0614e-02,  5.0093e-01, -3.4946e-02,  5.4074e-01,\n",
      "         -2.4947e-01,  9.2041e-01, -9.4326e-02,  2.4143e-01,  7.9519e-01,\n",
      "          1.3293e-01, -1.6005e-01, -5.0294e-01,  2.0460e-02, -8.3706e-01,\n",
      "          2.7683e-02, -8.9536e-01,  9.0312e-01, -1.3633e-01,  1.9349e-01,\n",
      "          5.9952e-02, -1.7222e-01,  9.9999e-01,  4.7007e-01,  4.7884e-01,\n",
      "         -5.7444e-01,  8.1856e-01, -8.0511e-01, -6.2675e-01, -3.2137e-01,\n",
      "          8.0806e-02,  2.4104e-01, -7.9018e-02,  1.7919e-01, -9.2885e-01,\n",
      "         -2.6650e-01, -1.2899e-01, -9.6133e-01, -9.7457e-01,  3.6623e-01,\n",
      "          7.2937e-01, -3.4597e-02, -1.7428e-01, -5.2906e-01, -4.3465e-01,\n",
      "          1.5120e-01, -1.0488e-01, -8.8355e-01,  4.6888e-01, -1.4193e-01,\n",
      "          3.8614e-01, -1.5423e-01,  2.7975e-01, -3.3387e-01,  7.8142e-01,\n",
      "          5.8021e-01,  1.6519e-01, -3.0625e-02, -7.2807e-01,  6.7642e-01,\n",
      "         -6.9032e-01,  2.6930e-01, -5.0710e-02,  1.0000e+00, -3.4446e-01,\n",
      "         -1.5852e-01,  5.9284e-01,  6.8333e-01, -1.3176e-02,  2.8753e-02,\n",
      "         -1.2258e-01,  1.0851e-01,  1.9791e-01,  3.0359e-01, -7.8865e-01,\n",
      "         -1.8382e-01,  3.8574e-01, -5.0675e-01, -3.8643e-01,  6.1179e-01,\n",
      "          7.9314e-02,  8.9012e-02,  3.4575e-02,  3.1832e-03,  9.9730e-01,\n",
      "         -1.1324e-01, -1.4074e-01, -3.9646e-01,  1.2135e-01, -3.0035e-01,\n",
      "         -5.5978e-01,  9.9993e-01,  2.5790e-01, -2.6066e-01, -9.6879e-01,\n",
      "          1.9145e-01, -8.1919e-01,  9.9171e-01,  7.1468e-01, -7.3359e-01,\n",
      "          4.2763e-01,  3.2264e-01, -1.1131e-01,  6.6721e-01, -7.4914e-02,\n",
      "         -1.6007e-01,  1.3054e-01,  7.5210e-02,  9.0491e-01, -3.2931e-01,\n",
      "         -9.0784e-01, -3.8525e-01,  2.0435e-01, -9.0458e-01,  8.0311e-01,\n",
      "         -3.7846e-01, -6.3642e-02, -1.6230e-01,  3.4892e-01,  7.8172e-01,\n",
      "         -7.9932e-02, -9.5081e-01, -8.0297e-02, -3.0591e-02,  9.2207e-01,\n",
      "          8.4605e-02, -2.7916e-01, -8.6094e-01, -2.5888e-01,  1.9273e-02,\n",
      "          1.6605e-01, -8.8368e-01,  9.2811e-01, -9.5956e-01,  3.5194e-01,\n",
      "          9.9991e-01,  1.8371e-01, -6.3581e-01,  5.6698e-02, -3.0820e-01,\n",
      "          1.3986e-01,  1.4408e-01,  4.7542e-01, -9.0773e-01, -2.5396e-01,\n",
      "         -9.9497e-02,  2.5010e-01, -1.2368e-01,  3.8971e-01,  5.0882e-01,\n",
      "          1.1098e-01, -1.9284e-01, -4.4186e-01,  7.9142e-02,  2.9759e-01,\n",
      "          6.2564e-01, -2.4864e-01, -7.5804e-02,  1.3207e-02, -6.7801e-02,\n",
      "         -8.4637e-01, -1.6788e-01, -1.2826e-01, -9.4807e-01,  5.7209e-01,\n",
      "         -1.0000e+00, -3.2719e-01, -5.1044e-01, -1.6913e-01,  7.4074e-01,\n",
      "          1.2782e-01, -7.8003e-02, -5.2916e-01,  2.1164e-01,  7.4138e-01,\n",
      "          5.9651e-01, -1.3897e-01,  1.4483e-01, -4.9222e-01,  3.8900e-02,\n",
      "         -5.1609e-02,  2.5214e-01,  2.2646e-01,  6.0180e-01, -1.4660e-01,\n",
      "          1.0000e+00,  2.1487e-02, -3.0266e-01, -9.5334e-01,  1.9239e-01,\n",
      "         -1.2907e-01,  9.9875e-01, -8.4633e-01, -9.0141e-01,  1.1960e-01,\n",
      "         -3.8747e-01, -7.0615e-01,  1.2634e-01,  6.8957e-03, -6.0779e-01,\n",
      "         -2.2246e-01,  9.0985e-01,  8.4395e-01, -3.0462e-01,  1.8918e-01,\n",
      "         -2.2558e-01, -3.8150e-01, -3.9259e-03, -2.8016e-01,  9.6889e-01,\n",
      "          1.0742e-01,  7.9554e-01,  4.1204e-01,  1.1007e-01,  9.0882e-01,\n",
      "          1.3899e-01,  4.8688e-01,  2.3080e-03,  9.9994e-01,  1.4957e-01,\n",
      "         -8.5762e-01,  3.6427e-01, -9.6703e-01, -5.7397e-02, -9.2712e-01,\n",
      "          1.7834e-01, -5.0661e-02,  7.7514e-01, -1.6596e-01,  8.9765e-01,\n",
      "          3.5659e-01,  5.4921e-02, -3.0219e-02,  5.8672e-01,  2.7135e-01,\n",
      "         -8.2813e-01, -9.6068e-01, -9.6523e-01,  2.6612e-01, -3.1534e-01,\n",
      "          1.0412e-02,  1.8158e-01,  1.4571e-01,  2.3417e-01,  2.8984e-01,\n",
      "         -9.9988e-01,  8.6468e-01,  2.5478e-01, -2.0296e-01,  9.1180e-01,\n",
      "          3.1731e-01,  1.8693e-01,  1.4892e-01, -9.6226e-01, -9.3863e-01,\n",
      "         -2.5176e-01, -2.1845e-01,  5.7887e-01,  4.4633e-01,  7.7534e-01,\n",
      "          2.6126e-01, -3.9340e-01, -1.0021e-01,  3.7026e-01, -4.3676e-01,\n",
      "         -9.8073e-01,  3.0337e-01,  1.8538e-01, -9.3134e-01,  8.9322e-01,\n",
      "         -3.2823e-01, -1.0006e-01,  4.7728e-01,  1.1967e-01,  9.0917e-01,\n",
      "          6.6472e-01,  3.8736e-01,  9.5587e-02,  4.0457e-01,  8.2336e-01,\n",
      "          8.9563e-01,  9.6024e-01,  9.8070e-02,  6.2209e-01,  2.5472e-01,\n",
      "          2.4166e-01,  2.4351e-01, -8.8428e-01,  3.0446e-02, -1.6451e-01,\n",
      "          3.6549e-02,  1.5941e-01, -8.3502e-02, -9.4306e-01, -1.0638e-01,\n",
      "         -1.5786e-01,  3.7088e-01, -2.6363e-01,  1.3906e-01, -2.6960e-01,\n",
      "         -7.2153e-02, -4.9791e-01, -5.0206e-01,  3.8268e-01,  1.8170e-01,\n",
      "          8.2497e-01,  1.5593e-01,  2.4760e-03, -4.5906e-01, -1.2441e-01,\n",
      "          2.8847e-01, -8.4349e-01,  8.5297e-01,  6.0898e-02,  3.9644e-01,\n",
      "         -4.3794e-01, -8.9386e-02,  2.1669e-01, -3.7349e-01, -2.7967e-01,\n",
      "         -2.3145e-01, -5.6763e-01,  7.4753e-01,  2.9057e-02, -3.1522e-01,\n",
      "         -3.9539e-01,  4.8848e-01,  2.2091e-01,  8.8854e-01,  1.1486e-01,\n",
      "         -5.0144e-02,  8.6642e-03, -1.8786e-01,  1.0328e-01, -1.4253e-02,\n",
      "         -9.9988e-01,  2.5597e-01,  9.3149e-02, -1.3983e-01, -1.1037e-03,\n",
      "         -2.0459e-01, -4.9012e-02, -9.4938e-01, -8.1150e-03, -1.5641e-01,\n",
      "         -1.8525e-01, -4.7469e-01, -3.6845e-01,  2.5859e-01,  3.3802e-01,\n",
      "          3.4060e-01,  7.9668e-01,  4.5850e-01,  5.1818e-01,  3.3453e-01,\n",
      "          1.9076e-01, -4.9731e-01,  8.3776e-01]])\n",
      "tensor([[[-0.0157, -0.1467,  0.1731,  ..., -0.3865,  0.0482, -0.0472],\n",
      "         [ 0.1676, -0.3134, -0.2382,  ..., -0.1375,  0.6669, -0.5144],\n",
      "         [-0.2030, -0.9535,  0.4619,  ...,  0.3489,  0.3485,  0.1053],\n",
      "         [-0.8408, -0.5504,  0.3944,  ..., -1.0462,  0.1121,  0.2073],\n",
      "         [ 0.2167, -0.5088, -0.7847,  ..., -0.0968, -0.0144, -0.3253],\n",
      "         [ 0.4949, -0.1485, -0.1216,  ...,  0.1164, -0.5264, -0.1917]]])\n"
     ]
    }
   ],
   "source": [
    "# # Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "b = embed_sentence_mean_layer11(\"awesome was this movie\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', '<', 'o', '##ov', '>', 'world']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('hello <oov> world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# dataset & loader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# net\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# optimiser\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"preprocess-data/aclImdb/imdb\"\n",
    "\n",
    "# make sure on cuda & efficient\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset & loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, file_name, embed_func, numrows=None):\n",
    "        self.file_data = pd.read_csv(file_name, nrows=numrows)\n",
    "        self.file_data = self.file_data.dropna()  # drop all nan values\n",
    "        self.embed_func = embed_func\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.file_data.iloc[idx, 0]\n",
    "        label = torch.from_numpy(self.file_data.iloc[idx, 1:].values.astype(np.int))\n",
    "        return self.embed_func(text), label.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "train_sent_dataset = SentenceDataset(data_path+\"_train_clean.csv\", lambda sent: embed_sentence_mean_layer11(sent, model, tokenizer))\n",
    "test_sent_dataset = SentenceDataset(data_path+\"_test_clean.csv\", lambda sent: embed_sentence_mean_layer11(sent, model, tokenizer))\n",
    "\n",
    "# dataloaders\n",
    "train_dataloader = DataLoader(train_sent_dataset, batch_size=32, num_workers=8, shuffle=True)\n",
    "valid_dataloader = DataLoader(test_sent_dataset, batch_size=32, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this shows data and label shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([32, 1, 768]) torch.Size([32, 1])\n",
      "1 torch.Size([32, 1, 768]) torch.Size([32, 1])\n",
      "2 torch.Size([32, 1, 768]) torch.Size([32, 1])\n",
      "3 torch.Size([32, 1, 768]) torch.Size([32, 1])\n",
      "4 torch.Size([32, 1, 768]) torch.Size([32, 1])\n",
      "5 torch.Size([32, 1, 768]) torch.Size([32, 1])\n",
      "6 torch.Size([32, 1, 768]) torch.Size([32, 1])\n",
      "7 torch.Size([32, 1, 768]) torch.Size([32, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6e98f655bbca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/conda_envs/bert_test/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/conda_envs/bert_test/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/conda_envs/bert_test/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/conda_envs/bert_test/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/conda_envs/bert_test/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/conda_envs/bert_test/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/conda_envs/bert_test/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/conda_envs/bert_test/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    print(i, data[0].shape, data[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### net and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=768, out_features=400, bias=True)\n",
       "  (fc2): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 400)\n",
    "        self.fc2 = nn.Linear(400, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 768)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)  # softmax is built in to loss\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics():\n",
    "    running_accuracy = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(valid_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward\n",
    "        inputs = inputs.type(dtype)\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            outputs = net(inputs)\n",
    "        labels = labels.type(torch.cuda.LongTensor)\n",
    "        loss = criterion(outputs, labels.view(-1))\n",
    "\n",
    "        _, preds = outputs.max(1)\n",
    "        running_accuracy += ((preds == labels.view(-1)).sum().to(dtype=torch.float)/len(outputs)).item()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    # loss\n",
    "    print('[%d, %5d] val loss: %.3f' %\n",
    "          (1, i + 1, running_loss / (i+1)))\n",
    "\n",
    "    # accuracy\n",
    "    print('[%d, %5d] val accuracy: %.3f' %\n",
    "          (1, i + 1, running_accuracy / (i+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.286\n",
      "[1,    50] accuracy: 0.881\n",
      "[1,   100] loss: 0.260\n",
      "[1,   100] accuracy: 0.892\n",
      "[1,   150] loss: 0.271\n",
      "[1,   150] accuracy: 0.884\n",
      "[1,   200] loss: 0.276\n",
      "[1,   200] accuracy: 0.888\n",
      "[1,   250] loss: 0.264\n",
      "[1,   250] accuracy: 0.893\n",
      "[1,   300] loss: 0.274\n",
      "[1,   300] accuracy: 0.886\n",
      "[1,   350] loss: 0.288\n",
      "[1,   350] accuracy: 0.882\n",
      "[1,   400] loss: 0.278\n",
      "[1,   400] accuracy: 0.887\n",
      "[1,   450] loss: 0.271\n",
      "[1,   450] accuracy: 0.891\n",
      "[1,   464] val loss: 0.276\n",
      "[1,   464] val accuracy: 0.885\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_accuracy = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        inputs = inputs.type(dtype)\n",
    "        outputs = net(inputs)\n",
    "        labels = labels.type(torch.cuda.LongTensor)\n",
    "        loss = criterion(outputs, labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = outputs.max(1)\n",
    "        running_accuracy += ((preds == labels.view(-1)).sum().to(dtype=torch.float)/len(outputs)).item()\n",
    "        \n",
    "        \n",
    "        # print statistics\n",
    "        n = 50\n",
    "        running_loss += loss.item()\n",
    "        if i % n == n-1:    # print every n mini-batches\n",
    "            # loss\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / n))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            # accuracy\n",
    "            print('[%d, %5d] accuracy: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_accuracy / n))\n",
    "            running_accuracy = 0.0\n",
    "    val_metrics()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.283\n",
      "[1,    50] accuracy: 0.886\n",
      "[1,   100] loss: 0.271\n",
      "[1,   100] accuracy: 0.889\n",
      "[1,   150] loss: 0.257\n",
      "[1,   150] accuracy: 0.894\n",
      "[1,   200] loss: 0.284\n",
      "[1,   200] accuracy: 0.882\n",
      "[1,   250] loss: 0.273\n",
      "[1,   250] accuracy: 0.891\n",
      "[1,   300] loss: 0.274\n",
      "[1,   300] accuracy: 0.877\n",
      "[1,   350] loss: 0.279\n",
      "[1,   350] accuracy: 0.888\n",
      "[1,   400] loss: 0.261\n",
      "[1,   400] accuracy: 0.893\n",
      "[1,   450] loss: 0.262\n",
      "[1,   450] accuracy: 0.896\n",
      "[1,   464] val loss: 0.277\n",
      "[1,   464] val accuracy: 0.885\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_accuracy = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        inputs = inputs.type(dtype)\n",
    "        outputs = net(inputs)\n",
    "        labels = labels.type(torch.cuda.LongTensor)\n",
    "        loss = criterion(outputs, labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = outputs.max(1)\n",
    "        running_accuracy += ((preds == labels.view(-1)).sum().to(dtype=torch.float)/len(outputs)).item()\n",
    "        \n",
    "        \n",
    "        # print statistics\n",
    "        n = 50\n",
    "        running_loss += loss.item()\n",
    "        if i % n == n-1:    # print every n mini-batches\n",
    "            # loss\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / n))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            # accuracy\n",
    "            print('[%d, %5d] accuracy: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_accuracy / n))\n",
    "            running_accuracy = 0.0\n",
    "    val_metrics()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'preprocess-data/aclImdb/imdb.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed_sent = embed_sentence_mean_layer11(\"I wouldn't wait one more second to see this movie, nor would I reccomnd you to\", model, tokenizer)\n",
    "# embed_sent = embed_sentence_mean_layer11(\"I have never experienced glee, nor happiness as passionately as in this movie\", model, tokenizer)\n",
    "embed_sent = embed_sentence_mean_layer11(\"are creatures born , or made ?\", model, tokenizer)\n",
    "embed_sent = embed_sent.type(dtype)\n",
    "net.load_state_dict(torch.load('preprocess-data/rt-polaritydata/polarity.pth'))\n",
    "F.softmax(net(embed_sent).detach().cpu()).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99025303,  0.92271733]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_sent = embed_sentence_mean_layer11(\"good\", model, tokenizer)\n",
    "embed_sent = embed_sent.type(dtype)\n",
    "net(embed_sent).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'preprocess-data/aclImdb/imdb'\n",
    "model_path = base_path + '.pth'\n",
    "tst_path = base_path + '_test_clean.csv'\n",
    "out_path = base_path + '_test_pred.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_df = pd.read_csv(tst_path)\n",
    "tst_df = tst_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14835, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7120</th>\n",
       "      <td>ah , how refreshing to see a vision of 18th ce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5224</th>\n",
       "      <td>what a trip down memory lane . do not look for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>although it is more of a kids movie , it still...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>at first , i honestly thought it would be a co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7547</th>\n",
       "      <td>james cagney plays richard gargan ( nicknamed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  label\n",
       "7120  ah , how refreshing to see a vision of 18th ce...      1\n",
       "5224  what a trip down memory lane . do not look for...      1\n",
       "562   although it is more of a kids movie , it still...      1\n",
       "1470  at first , i honestly thought it would be a co...      1\n",
       "7547  james cagney plays richard gargan ( nicknamed ...      0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tst_df.shape)\n",
    "tst_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "test_sent_dataset = SentenceDataset(tst_path, lambda sent: embed_sentence_mean_layer11(sent, model, tokenizer))\n",
    "\n",
    "# dataloaders\n",
    "valid_dataloader = DataLoader(test_sent_dataset, batch_size=32, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "\n",
    "for data in valid_dataloader:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        inputs = inputs.type(dtype)\n",
    "        with torch.no_grad():\n",
    "            outputs = net(inputs)\n",
    "        _, preds = outputs.max(1)\n",
    "        pred_list.append(preds.cpu().numpy())\n",
    "        \n",
    "tst_df['preds'] = np.concatenate(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8848668688911359"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tst_df['preds']==tst_df['label']).sum()/len(tst_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preprocess-data/aclImdb/imdb_test_pred.csv'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "test2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
