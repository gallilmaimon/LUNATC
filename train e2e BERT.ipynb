{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/toxic/toxic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(data_path+\"_train_clean.csv\")\n",
    "df_test = pd.read_csv(data_path+\"_test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww ! he matches this background colour i'm ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man , i'm really not trying to edit war . ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" more i can't make any real suggestions on im...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you , sir , are my hero . any chance you remem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0  explanation why the edits made under my userna...      0\n",
       "1  d'aww ! he matches this background colour i'm ...      0\n",
       "2  hey man , i'm really not trying to edit war . ...      0\n",
       "3  \" more i can't make any real suggestions on im...      0\n",
       "4  you , sir , are my hero . any chance you remem...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "sentences_train = df_train.content.values\n",
    "labels_train = df_train.label.values\n",
    "\n",
    "sentences_test = df_test.content.values\n",
    "labels_test = df_test.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenising & formatting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0601 12:32:36.373601 140251914221312 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "W0601 12:32:36.987436 140251914221312 __init__.py:28] To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n",
      "I0601 12:32:37.422487 140251914221312 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0601 12:32:38.329735 140251914221312 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/gallil/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0519 14:46:48.197519 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:46:49.220181 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:01.571424 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (3731 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:05.309104 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:06.396987 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1318 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:08.521259 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (4643 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:14.785989 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:16.406631 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:16.778701 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:19.790167 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1421 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:21.000476 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (4948 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:29.520933 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:39.134174 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:43.226422 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:51.138157 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:53.687461 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (4855 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:55.133872 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:47:55.693205 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1273 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:00.430143 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:05.076785 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:06.804905 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:07.090673 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:07.578498 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (737 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:07.704670 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (2732 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:09.136643 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (706 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:09.253658 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:10.465016 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (684 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:11.320628 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:11.583296 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (3305 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:13.434196 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (4948 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:13.608573 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (904 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:13.848848 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:14.924759 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:16.600122 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (921 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0519 14:48:18.213428 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (888 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:18.919392 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1135 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:19.110232 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:19.529820 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:19.790699 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:20.118364 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1008 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:20.149833 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:20.852118 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:22.281987 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:22.340893 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:22.354344 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:22.874307 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:22.905690 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1374 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:23.160863 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (719 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:23.287817 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:23.328095 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:24.167008 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:24.216824 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (735 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:24.639235 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1255 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:25.673692 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1283 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:27.065648 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:27.530105 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:27.650504 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:29.158760 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:29.728927 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:30.791727 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (857 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:31.025123 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1601 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:32.065554 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:32.363275 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (783 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:32.868067 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:32.952577 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (600 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:32.994687 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (796 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:33.504484 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:33.832170 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0519 14:48:34.484004 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1022 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:34.495610 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (908 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:34.643521 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:34.874678 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:36.270719 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (830 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:37.225905 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:38.057059 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (1133 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:38.176885 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:38.680421 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:38.856773 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (925 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0519 14:48:39.800231 140148923213568 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (2341 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_train = []\n",
    "\n",
    "# For every sentence in train\n",
    "for sent in sentences_train:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids_train.append(encoded_sent)\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_test = []\n",
    "\n",
    "# For every sentence in test\n",
    "for sent in sentences_test:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids_test.append(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(input_ids, maxlen):\n",
    "    padded = []\n",
    "    for inp in input_ids:\n",
    "        if len(inp) >= maxlen:\n",
    "            padded.append(inp[:maxlen-1] + [inp[-1]])\n",
    "        else:\n",
    "            padded.append(inp + [0]*(maxlen - len(inp)))\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### histogram of length for choosing the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0519 14:48:41.153230 140148923213568 font_manager.py:1349] generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens_train = [len(inp) for inp in  input_ids_train]\n",
    "plt.hist(lens_train, bins=1000, range=(0,1000))\n",
    "plt.show()\n",
    "lens_test = [len(inp) for inp in  input_ids_test]\n",
    "plt.hist(lens_test, bins=1000, range=(0,1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51861\n"
     ]
    }
   ],
   "source": [
    "print((np.array(lens_test)<=128).sum()) # / len(lens_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 128 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum sequence length.\n",
    "MAX_LEN = 128\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "input_ids_train = pad_sequences(input_ids_train, maxlen=MAX_LEN)\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN)\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks_train = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids_train:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks_train.append(att_mask)\n",
    "    \n",
    "# Create attention masks\n",
    "attention_masks_test = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids_test:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks_test.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = input_ids_train, input_ids_test, labels_train, labels_test \n",
    "train_masks, validation_masks = attention_masks_train, attention_masks_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all inputs and labels into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 19300864)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.element_size(), train_inputs.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150788, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model & optimiser & scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0519 14:48:49.317243 140148923213568 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/gallil/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0519 14:48:49.320569 140148923213568 configuration_utils.py:168] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0519 14:48:50.330006 140148923213568 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/gallil/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0519 14:48:52.382200 140148923213568 modeling_utils.py:405] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0519 14:48:52.382731 140148923213568 modeling_utils.py:408] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WarmupLinearSchedule\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = WarmupLinearSchedule(optimizer, \n",
    "                                 warmup_steps = 0, # Default value in run_glue.py\n",
    "                                 t_total = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return (pred_flat == labels_flat).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.86\n",
      "  Validation took: 0:03:25\n"
     ]
    }
   ],
   "source": [
    "# evaluation only - to make sure that accuracy is more or less random at the beginnig\n",
    "print(\"\")\n",
    "print(\"Running Validation...\")\n",
    "device = \"cuda\"\n",
    "t0 = time.time()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():        \n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "    # values prior to applying an activation function like the softmax.\n",
    "    logits = outputs[0]\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences.\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # Accumulate the total accuracy.\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    # Track the number of batches\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "# Report the final accuracy for this validation run.\n",
    "print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  4,713.    Elapsed: 0:00:14.\n",
      "  Batch    80  of  4,713.    Elapsed: 0:00:27.\n",
      "  Batch   120  of  4,713.    Elapsed: 0:00:40.\n",
      "  Batch   160  of  4,713.    Elapsed: 0:00:54.\n",
      "  Batch   200  of  4,713.    Elapsed: 0:01:07.\n",
      "  Batch   240  of  4,713.    Elapsed: 0:01:21.\n",
      "  Batch   280  of  4,713.    Elapsed: 0:01:34.\n",
      "  Batch   320  of  4,713.    Elapsed: 0:01:49.\n",
      "  Batch   360  of  4,713.    Elapsed: 0:02:03.\n",
      "  Batch   400  of  4,713.    Elapsed: 0:02:18.\n",
      "  Batch   440  of  4,713.    Elapsed: 0:02:33.\n",
      "  Batch   480  of  4,713.    Elapsed: 0:02:47.\n",
      "  Batch   520  of  4,713.    Elapsed: 0:03:01.\n",
      "  Batch   560  of  4,713.    Elapsed: 0:03:16.\n",
      "  Batch   600  of  4,713.    Elapsed: 0:03:30.\n",
      "  Batch   640  of  4,713.    Elapsed: 0:03:44.\n",
      "  Batch   680  of  4,713.    Elapsed: 0:03:58.\n",
      "  Batch   720  of  4,713.    Elapsed: 0:04:13.\n",
      "  Batch   760  of  4,713.    Elapsed: 0:04:27.\n",
      "  Batch   800  of  4,713.    Elapsed: 0:04:42.\n",
      "  Batch   840  of  4,713.    Elapsed: 0:04:56.\n",
      "  Batch   880  of  4,713.    Elapsed: 0:05:11.\n",
      "  Batch   920  of  4,713.    Elapsed: 0:05:25.\n",
      "  Batch   960  of  4,713.    Elapsed: 0:05:39.\n",
      "  Batch 1,000  of  4,713.    Elapsed: 0:05:54.\n",
      "  Batch 1,040  of  4,713.    Elapsed: 0:06:08.\n",
      "  Batch 1,080  of  4,713.    Elapsed: 0:06:23.\n",
      "  Batch 1,120  of  4,713.    Elapsed: 0:06:37.\n",
      "  Batch 1,160  of  4,713.    Elapsed: 0:06:52.\n",
      "  Batch 1,200  of  4,713.    Elapsed: 0:07:06.\n",
      "  Batch 1,240  of  4,713.    Elapsed: 0:07:20.\n",
      "  Batch 1,280  of  4,713.    Elapsed: 0:07:35.\n",
      "  Batch 1,320  of  4,713.    Elapsed: 0:07:49.\n",
      "  Batch 1,360  of  4,713.    Elapsed: 0:08:03.\n",
      "  Batch 1,400  of  4,713.    Elapsed: 0:08:18.\n",
      "  Batch 1,440  of  4,713.    Elapsed: 0:08:33.\n",
      "  Batch 1,480  of  4,713.    Elapsed: 0:08:47.\n",
      "  Batch 1,520  of  4,713.    Elapsed: 0:09:02.\n",
      "  Batch 1,560  of  4,713.    Elapsed: 0:09:16.\n",
      "  Batch 1,600  of  4,713.    Elapsed: 0:09:30.\n",
      "  Batch 1,640  of  4,713.    Elapsed: 0:09:45.\n",
      "  Batch 1,680  of  4,713.    Elapsed: 0:09:59.\n",
      "  Batch 1,720  of  4,713.    Elapsed: 0:10:14.\n",
      "  Batch 1,760  of  4,713.    Elapsed: 0:10:29.\n",
      "  Batch 1,800  of  4,713.    Elapsed: 0:10:43.\n",
      "  Batch 1,840  of  4,713.    Elapsed: 0:10:59.\n",
      "  Batch 1,880  of  4,713.    Elapsed: 0:11:13.\n",
      "  Batch 1,920  of  4,713.    Elapsed: 0:11:28.\n",
      "  Batch 1,960  of  4,713.    Elapsed: 0:11:42.\n",
      "  Batch 2,000  of  4,713.    Elapsed: 0:11:56.\n",
      "  Batch 2,040  of  4,713.    Elapsed: 0:12:11.\n",
      "  Batch 2,080  of  4,713.    Elapsed: 0:12:25.\n",
      "  Batch 2,120  of  4,713.    Elapsed: 0:12:39.\n",
      "  Batch 2,160  of  4,713.    Elapsed: 0:12:54.\n",
      "  Batch 2,200  of  4,713.    Elapsed: 0:13:08.\n",
      "  Batch 2,240  of  4,713.    Elapsed: 0:13:22.\n",
      "  Batch 2,280  of  4,713.    Elapsed: 0:13:37.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "device = \"cuda:0\"\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    print()\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull theloss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0 - This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), open(data_path+\"e2e_bert.pth\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0319 10:08:26.287150 140144781510400 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/gallil/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0319 10:08:27.398584 140144781510400 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/gallil/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0319 10:08:27.402496 140144781510400 configuration_utils.py:168] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0319 10:08:28.209434 140144781510400 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/gallil/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "# with a saved model & cpu - Load a trained model that you have fine-tuned\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "model_loaded = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", state_dict=torch.load(data_path+\"e2e_bert.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "text = 'it was tedious at times but altogether i would recommend it'\n",
    "model_loaded.eval()\n",
    "with torch.no_grad():\n",
    "    sent_token = torch.Tensor(pad_sequences([tokenizer.encode(text, add_special_tokens=True)], 128)).long()\n",
    "    sent_att = (sent_token > 0).int()\n",
    "    res = model_loaded(sent_token, attention_mask=sent_att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## infer on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/toxic/toxic'\n",
    "model_path = base_path + 'e2e_bert.pth'\n",
    "tst_path = base_path + '_test_clean.csv'\n",
    "out_path = base_path + '_test_pred.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0519 14:55:26.958632 140148923213568 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/gallil/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0519 14:55:26.962064 140148923213568 configuration_utils.py:168] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0519 14:55:27.814405 140148923213568 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/gallil/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", state_dict=torch.load(model_path))\n",
    "model_loaded.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rad the dataset\n",
    "tst_df = pd.read_csv(tst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/conda_envs/left/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "proba_list = []\n",
    "# evaluation only\n",
    "print(\"\")\n",
    "print(\"Running Validation...\")\n",
    "device = \"cuda\"\n",
    "t0 = time.time()\n",
    "\n",
    "model_loaded.eval()\n",
    "\n",
    "# Tracking variables \n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():        \n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model_loaded(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # Get the \"logits\" output by the model\n",
    "    logits = outputs[0]\n",
    "    \n",
    "    probs, preds = F.softmax(logits).max(1)\n",
    "    \n",
    "    pred_list.append(preds.cpu().numpy())\n",
    "    proba_list.append(probs.cpu().numpy())\n",
    "    \n",
    "tst_df['preds'] = np.concatenate(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# look at distibution of probabilities\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(np.concatenate(proba_list), bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9144682537062105"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure of accuracy and no mistakes\n",
    "(tst_df['preds']==tst_df['label']).sum()/len(tst_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "tst_df.to_csv(out_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
