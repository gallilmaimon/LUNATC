{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "device = \"cuda:0\"\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/aclImdb/imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path+\"_train_clean.csv\")\n",
    "df_train, df_validation = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(data_path+\"_test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "sentences_train = df_train.content.values\n",
    "labels_train = df_train.label.values\n",
    "\n",
    "sentences_validation = df_validation.content.values\n",
    "labels_validation = df_validation.label.values\n",
    "\n",
    "sentences_test = df_test.content.values\n",
    "labels_test = df_test.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenising & formatting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gallil/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gallil/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gallil/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gallil/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gallil/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gallil/anaconda3/envs/test2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XLNET tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading XLNET tokenizer...')\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_train = []\n",
    "\n",
    "# For every sentence in train\n",
    "for sent in sentences_train:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids_train.append(encoded_sent)\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_validation = []\n",
    "\n",
    "# For every sentence in test\n",
    "for sent in sentences_validation:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids_validation.append(encoded_sent)\n",
    "    \n",
    "    \n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_test = []\n",
    "\n",
    "# For every sentence in test\n",
    "for sent in sentences_test:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids_test.append(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(input_ids, maxlen):\n",
    "    padded = []\n",
    "    for inp in input_ids:\n",
    "        if len(inp) >= maxlen:\n",
    "            padded.append(inp[:maxlen-1] + [inp[-1]])\n",
    "        else:\n",
    "            padded.append(inp + [0]*(maxlen - len(inp)))\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### histogram of length for choosing the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-6q4k8m1m because the default path (/home/gallil/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens_train = [len(inp) for inp in  input_ids_train]\n",
    "plt.hist(lens_train, bins=1000, range=(0,1000))\n",
    "plt.show()\n",
    "lens_validation = [len(inp) for inp in  input_ids_validation]\n",
    "plt.hist(lens_validation, bins=1000, range=(0,1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8399542508577964\n"
     ]
    }
   ],
   "source": [
    "print((np.array(lens_train)<=256).sum()/ len(lens_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding/truncating all sentences to 256 values...\n",
      "Padding token: \"<pad>\", ID: 5\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum sequence length.\n",
    "MAX_LEN = 256\n",
    "\n",
    "print('Padding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "print('Padding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "input_ids_train = pad_sequences(input_ids_train, maxlen=MAX_LEN)\n",
    "input_ids_validation = pad_sequences(input_ids_validation, maxlen=MAX_LEN)\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks_train = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids_train:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks_train.append(att_mask)\n",
    "    \n",
    "# Create attention masks\n",
    "attention_masks_validation = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids_validation:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks_validation.append(att_mask)\n",
    "\n",
    "    \n",
    "attention_masks_test = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids_test:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks_test.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename\n",
    "train_inputs, validation_inputs, test_inputs, train_labels, validation_labels, test_labels = input_ids_train, input_ids_validation, input_ids_test, labels_train, labels_validation, labels_test\n",
    "train_masks, validation_masks, test_masks = attention_masks_train, attention_masks_validation, attention_masks_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all inputs and labels into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13115, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model & optimiser & scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetForSequenceClassification, AdamW, XLNetConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dacf7356e9cd4e22b85171ebcb6944b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=760.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gallil/anaconda3/envs/test2/lib/python3.7/site-packages/transformers/configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `mem_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c568878322f047df849d51d970bfa5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=467042463.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = XLNetForSequenceClassification.from_pretrained(\n",
    "    \"xlnet-base-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2,\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, \n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return (pred_flat == labels_flat).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.49\n",
      "  Validation took: 0:00:20\n"
     ]
    }
   ],
   "source": [
    "# evaluation only - to make sure that accuracy is more or less random at the beginnig\n",
    "print(\"\")\n",
    "print(\"Running Validation...\")\n",
    "device = \"cuda\"\n",
    "t0 = time.time()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():        \n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "    # values prior to applying an activation function like the softmax.\n",
    "    logits = outputs[0]\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences.\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # Accumulate the total accuracy.\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    # Track the number of batches\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "# Report the final accuracy for this validation run.\n",
    "print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of    820.    Elapsed: 0:00:20.\n",
      "  Batch    80  of    820.    Elapsed: 0:00:41.\n",
      "  Batch   120  of    820.    Elapsed: 0:01:02.\n",
      "  Batch   160  of    820.    Elapsed: 0:01:23.\n",
      "  Batch   200  of    820.    Elapsed: 0:01:45.\n",
      "  Batch   240  of    820.    Elapsed: 0:02:07.\n",
      "  Batch   280  of    820.    Elapsed: 0:02:29.\n",
      "  Batch   320  of    820.    Elapsed: 0:02:52.\n",
      "  Batch   360  of    820.    Elapsed: 0:03:14.\n",
      "  Batch   400  of    820.    Elapsed: 0:03:36.\n",
      "  Batch   440  of    820.    Elapsed: 0:03:58.\n",
      "  Batch   480  of    820.    Elapsed: 0:04:20.\n",
      "  Batch   520  of    820.    Elapsed: 0:04:42.\n",
      "  Batch   560  of    820.    Elapsed: 0:05:04.\n",
      "  Batch   600  of    820.    Elapsed: 0:05:26.\n",
      "  Batch   640  of    820.    Elapsed: 0:05:48.\n",
      "  Batch   680  of    820.    Elapsed: 0:06:11.\n",
      "  Batch   720  of    820.    Elapsed: 0:06:33.\n",
      "  Batch   760  of    820.    Elapsed: 0:06:55.\n",
      "  Batch   800  of    820.    Elapsed: 0:07:17.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epcoh took: 0:07:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9450\n",
      "  Validation took: 0:00:22\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of    820.    Elapsed: 0:00:22.\n",
      "  Batch    80  of    820.    Elapsed: 0:00:45.\n",
      "  Batch   120  of    820.    Elapsed: 0:01:07.\n",
      "  Batch   160  of    820.    Elapsed: 0:01:28.\n",
      "  Batch   200  of    820.    Elapsed: 0:01:49.\n",
      "  Batch   240  of    820.    Elapsed: 0:02:10.\n",
      "  Batch   280  of    820.    Elapsed: 0:02:31.\n",
      "  Batch   320  of    820.    Elapsed: 0:02:52.\n",
      "  Batch   360  of    820.    Elapsed: 0:03:13.\n",
      "  Batch   400  of    820.    Elapsed: 0:03:35.\n",
      "  Batch   440  of    820.    Elapsed: 0:03:56.\n",
      "  Batch   480  of    820.    Elapsed: 0:04:17.\n",
      "  Batch   520  of    820.    Elapsed: 0:04:38.\n",
      "  Batch   560  of    820.    Elapsed: 0:04:59.\n",
      "  Batch   600  of    820.    Elapsed: 0:05:20.\n",
      "  Batch   640  of    820.    Elapsed: 0:05:41.\n",
      "  Batch   680  of    820.    Elapsed: 0:06:02.\n",
      "  Batch   720  of    820.    Elapsed: 0:06:24.\n",
      "  Batch   760  of    820.    Elapsed: 0:06:45.\n",
      "  Batch   800  of    820.    Elapsed: 0:07:07.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epcoh took: 0:07:18\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9450\n",
      "  Validation took: 0:00:22\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    print()\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull theloss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0 - This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), open(data_path+\"_xlnet.pth\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a saved model & cpu - Load a trained model that you have fine-tuned\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "model_loaded = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", state_dict=torch.load(data_path+\"e2e_bert.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "text = 'it was tedious at times but altogether i would recommend it'\n",
    "model_loaded.eval()\n",
    "with torch.no_grad():\n",
    "    sent_token = torch.Tensor(pad_sequences([tokenizer.encode(text, add_special_tokens=True)], 128)).long()\n",
    "    sent_att = (sent_token > 0).int()\n",
    "    res = model_loaded(sent_token, attention_mask=sent_att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## infer on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/aclImdb/imdb'\n",
    "model_path = base_path + '_xlnet.pth'\n",
    "tst_path = base_path + '_test_clean.csv'\n",
    "out_path = base_path + '_test_pred_xlnet.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", state_dict=torch.load(model_path))\n",
    "model_loaded.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rad the dataset\n",
    "tst_df = pd.read_csv(tst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gallil/anaconda3/envs/test2/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "proba_list = []\n",
    "raw_out = []\n",
    "# evaluation only\n",
    "print(\"\")\n",
    "print(\"Running Test...\")\n",
    "device = \"cuda\"\n",
    "t0 = time.time()\n",
    "\n",
    "model_loaded.eval()\n",
    "\n",
    "# Tracking variables \n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():        \n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model_loaded(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # Get the \"logits\" output by the model\n",
    "    logits = outputs[0]\n",
    "    \n",
    "    raw_out.append(logits.cpu().numpy())\n",
    "    \n",
    "    probs, preds = F.softmax(logits).max(1)\n",
    "    \n",
    "    pred_list.append(preds.cpu().numpy())\n",
    "    proba_list.append(probs.cpu().numpy())\n",
    "    \n",
    "tst_df['preds'] = np.concatenate(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# look at distibution of probabilities\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(np.concatenate(proba_list), bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9511668690138945"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure of accuracy and no mistakes\n",
    "(tst_df['preds']==tst_df['label']).sum()/len(tst_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "tst_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### infernce with other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.TextModels.E2EBert as E2EBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.TextModels.E2EBert' from '/home/gallil/Desktop/projects/LUNATC/src/TextModels/E2EBert.py'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(E2EBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "code_model = E2EBert.BertTextModel(trained_model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177 ms ± 7.04 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "code_model.predict_proba(32*['amazing film !'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield ndx, iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ['thank you for understanding . i think very highly of you and would not revert without discussion .', ': dear god this site is horrible .', '\"::: somebody will invariably try to add religion ? really ?? you mean , the way people have invariably kept adding \"\" religion \"\" to the samuel beckett infobox ? and why do you bother bringing up the long - dead completely non - existent \"\" influences \"\" issue ? you\\'re just flailing , making up crap on the fly . ::: for comparison , the only explicit acknowledgement in the entire amos oz article that he is personally jewish is in the categories ! \"'])\n",
      "(3, ['\" it says it right there that it is a type . the \"\" type \"\" of institution is needed in this case because there are three levels of suny schools : - university centers and doctoral granting institutions - state colleges - community colleges . it is needed in this case to clarify that ub is a suny center . it says it even in binghamton university , university at albany , state university of new york , and stony brook university . stop trying to say it\\'s not because i am totally right in this case .\"', '\" = before adding a new product to the list , make sure it\\'s relevant = before adding a new product to the list , make sure it has a wikipedia entry already , \"\" proving \"\" it\\'s relevance and giving the reader the possibility to read more about it . otherwise it could be subject to deletion . see this article\\'s revision history .\"', 'this other one from 1897'])\n",
      "(6, ['= reason for banning throwing = this article needs a section on / why / throwing is banned . at the moment , to a non - cricket fan , it seems kind of arbitrary .', '|blocked ]] from editing wikipedia . |', '= arabs are committing genocide in iraq , but no protests in europe . = may europe also burn in hell .'])\n",
      "(9, ['please stop . if you continue to vandalize wikipedia , as you did to homosexuality , you will be blocked from editing .'])\n"
     ]
    }
   ],
   "source": [
    "for b in batch(tst_df.content.iloc[:10].values.tolist(), 3):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99998349126688016655"
     ]
    }
   ],
   "source": [
    "code_preds = []\n",
    "for i, text in enumerate(tst_df.content):\n",
    "    print(f'\\r{i/len(tst_df)}', end='')\n",
    "    code_preds.append(code_model.predict_proba(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995047380064054555"
     ]
    }
   ],
   "source": [
    "code_preds = []\n",
    "for i, text in batch(tst_df.content.values.tolist(), 64):\n",
    "    print(f'\\r{i/len(tst_df)}', end='')\n",
    "    code_preds.append(code_model.predict_proba(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs((np.concatenate(code_preds) - np.concatenate(raw_out))) > 0.0001).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60574, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(raw_out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "test2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
