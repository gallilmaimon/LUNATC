{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/toxic/toxic\"\n",
    "embedding_path = \"resources/word_vectors/glove.6B.200d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(data_path+\"_train_clean.csv\")\n",
    "df_test = pd.read_csv(data_path+\"_test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def balance_df(df):\n",
    "#     pos = df[df.label==1]\n",
    "#     neg = df[df.label==0].sample(len(pos))\n",
    "#     return pd.concat([pos,neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = balance_df(df_train)\n",
    "# df_test = balance_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(path):\n",
    "    words = [ ]\n",
    "    vals = [ ]\n",
    "    with open(path, encoding='utf-8') as fin:\n",
    "        fin.readline()\n",
    "        for line in fin:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                parts = line.split(' ')\n",
    "                words.append(parts[0])\n",
    "                vals += [float(x) for x in parts[1:]]\n",
    "    return words, np.asarray(vals).reshape(len(words),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, embeddings = load_embedding(embedding_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(inp, maxlen, token=0):\n",
    "    if len(inp) >= maxlen:\n",
    "        return inp[:maxlen-1] + [inp[-1]]\n",
    "    else:\n",
    "        return inp + [token]*(maxlen - len(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordLSTMDataset(Dataset):\n",
    "    def __init__(self, words, text_df, maxlen=128):\n",
    "        self.word2ind = defaultdict(lambda:len(words))  # this is the out of vocabulary token\n",
    "        self.maxlen = maxlen           # fixed length for padding sequences\n",
    "        self.pad_ind = len(words) + 1  # a padding index so that all inputs are the same length\n",
    "        self.word2ind.update({word:i for i,word in enumerate(words)})\n",
    "        self.text_df = text_df\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        row = self.text_df.iloc[i]\n",
    "        text, label = row.content, row.label\n",
    "        word_tokens = [self.word2ind[w] for w in text.split()]\n",
    "        return torch.Tensor(pad_sequences(word_tokens, self.maxlen, self.pad_ind)).long(), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = wordLSTMDataset(words, df_train)\n",
    "dataset_test = wordLSTMDataset(words, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, num_workers=8, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size=150, depth=1, dropout=0.3, nclasses=2, fix_emb=True,\n",
    "                 normalise=False):\n",
    "        super(WordLSTM, self).__init__()\n",
    "        \n",
    "        if normalise:\n",
    "            embedding /= np.linalg.norm(embedding,axis=1).reshape(-1, 1)\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(embedding.shape[0]+2, embedding.shape[1])\n",
    "        self.embedding.weight.data.uniform_(-0.25, 0.25)\n",
    "\n",
    "        self.embedding.weight.data[:len(embeddings)].copy_(torch.from_numpy(embeddings))\n",
    "        \n",
    "        if fix_emb:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding.shape[1], hidden_size//2, depth, dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, nclasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = self.drop(emb)\n",
    "        output, hidden = self.lstm(emb)\n",
    "        output = torch.tanh(self.bn(torch.max(output, dim=1)[0]))\n",
    "        output = self.drop(output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gallilm/.conda/envs/gallilm/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = WordLSTM(embeddings)\n",
    "device = 'cuda'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, dataloader, device):\n",
    "    running_accuracy = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs = model(inputs)\n",
    "        labels = labels.to(device)\n",
    "        loss = criterion(outputs, labels.view(-1))\n",
    "\n",
    "        _, preds = outputs.max(1)\n",
    "        running_accuracy += ((preds == labels.view(-1)).sum().to(dtype=torch.float)/len(outputs)).item()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    # loss\n",
    "    print('[%d, %5d] val loss: %.3f' %\n",
    "          (1, i + 1, running_loss / (i+1)))\n",
    "\n",
    "    # accuracy\n",
    "    print('[%d, %5d] val accuracy: %.3f' %\n",
    "          (1, i + 1, running_accuracy / (i+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.113\n",
      "[1,   200] accuracy: 0.959\n",
      "[1,   400] loss: 0.124\n",
      "[1,   400] accuracy: 0.953\n",
      "[1,   600] loss: 0.124\n",
      "[1,   600] accuracy: 0.954\n",
      "[1,   800] loss: 0.119\n",
      "[1,   800] accuracy: 0.953\n",
      "[1,  1000] loss: 0.115\n",
      "[1,  1000] accuracy: 0.955\n",
      "[1,  1200] loss: 0.116\n",
      "[1,  1200] accuracy: 0.956\n",
      "[1,  1400] loss: 0.116\n",
      "[1,  1400] accuracy: 0.957\n",
      "[1,  1600] loss: 0.115\n",
      "[1,  1600] accuracy: 0.957\n",
      "[1,  1800] loss: 0.117\n",
      "[1,  1800] accuracy: 0.960\n",
      "[1,  2000] loss: 0.119\n",
      "[1,  2000] accuracy: 0.957\n",
      "[1,  2200] loss: 0.107\n",
      "[1,  2200] accuracy: 0.962\n",
      "[1,  2400] loss: 0.122\n",
      "[1,  2400] accuracy: 0.955\n",
      "[1,  2600] loss: 0.115\n",
      "[1,  2600] accuracy: 0.956\n",
      "[1,  2800] loss: 0.128\n",
      "[1,  2800] accuracy: 0.951\n",
      "[1,  3000] loss: 0.114\n",
      "[1,  3000] accuracy: 0.958\n",
      "[1,  3200] loss: 0.128\n",
      "[1,  3200] accuracy: 0.952\n",
      "[1,  3400] loss: 0.112\n",
      "[1,  3400] accuracy: 0.958\n",
      "[1,  3600] loss: 0.115\n",
      "[1,  3600] accuracy: 0.958\n",
      "[1,  3800] loss: 0.119\n",
      "[1,  3800] accuracy: 0.955\n",
      "[1,  4000] loss: 0.116\n",
      "[1,  4000] accuracy: 0.957\n",
      "[1,  4200] loss: 0.127\n",
      "[1,  4200] accuracy: 0.954\n",
      "[1,  4400] loss: 0.105\n",
      "[1,  4400] accuracy: 0.958\n",
      "[1,  4600] loss: 0.115\n",
      "[1,  4600] accuracy: 0.954\n",
      "[1,  1893] val loss: 0.168\n",
      "[1,  1893] val accuracy: 0.929\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_accuracy = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader_train, 0):\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        labels = labels.to(device)\n",
    "        loss = criterion(outputs, labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = outputs.max(1)\n",
    "        running_accuracy += ((preds == labels.view(-1)).sum().to(dtype=torch.float)/len(outputs)).item()\n",
    "        \n",
    "        # print statistics\n",
    "        n = 200\n",
    "        running_loss += loss.item()\n",
    "        if i % n == n-1:    # print every n mini-batches\n",
    "            # loss\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / n))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            # accuracy\n",
    "            print('[%d, %5d] accuracy: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_accuracy / n))\n",
    "            running_accuracy = 0.0\n",
    "    val_metrics(model, dataloader_test, device)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), open(data_path+'_word_lstm.pth', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/clickbait/clickbait'\n",
    "model_path = base_path + '_word_lstm.pth'\n",
    "tst_path = base_path + '_test_clean.csv'\n",
    "out_path = base_path + '_test_pred_lstm.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File data/clickbait/clickbait_test_clean.csv does not exist: 'data/clickbait/clickbait_test_clean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bcf2b47831d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtst_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtst_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtst_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gallilm/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gallilm/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gallilm/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gallilm/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gallilm/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File data/clickbait/clickbait_test_clean.csv does not exist: 'data/clickbait/clickbait_test_clean.csv'"
     ]
    }
   ],
   "source": [
    "tst_df = pd.read_csv(tst_path)\n",
    "tst_df = tst_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "test_sent_dataset = wordLSTMDataset(words, tst_df)\n",
    "\n",
    "# dataloaders\n",
    "valid_dataloader = DataLoader(test_sent_dataset, batch_size=32, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "prob_list = []\n",
    "\n",
    "for data in valid_dataloader:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs = model(inputs)\n",
    "        _, preds = outputs.max(1)\n",
    "        pred_list.append(preds.cpu().numpy())\n",
    "        prob_list.append(outputs.cpu().numpy())\n",
    "        \n",
    "tst_df['preds'] = np.concatenate(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9322976854756166"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tst_df['preds']==tst_df['label']).sum()/len(tst_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8617968433832457"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tst_df['preds']==tst_df['label']).sum()/len(tst_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_df.to_csv(out_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gallilm",
   "language": "python",
   "name": "gallilm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
