{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed_val = 42\n",
    "device = \"cuda:0\"\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/aclImdb/imdb\"\n",
    "embedding_path = \"resources/word_vectors/glove.6B.200d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(data_path+\"_train_clean.csv\")\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.1)\n",
    "df_test = pd.read_csv(data_path+\"_test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_df(df):\n",
    "    pos = df[df.label==1]\n",
    "    neg = df[df.label==0].sample(len(pos))\n",
    "    return pd.concat([pos,neg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load GLOVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(path):\n",
    "    words = [ ]\n",
    "    vals = [ ]\n",
    "    with open(path, encoding='utf-8') as fin:\n",
    "        fin.readline()\n",
    "        for line in fin:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                parts = line.split(' ')\n",
    "                words.append(parts[0])\n",
    "                vals += [float(x) for x in parts[1:]]\n",
    "    return words, np.asarray(vals).reshape(len(words),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, embeddings = load_embedding(embedding_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(inp, maxlen, token=0):\n",
    "    if len(inp) >= maxlen:\n",
    "        return inp[:maxlen-1] + [inp[-1]]\n",
    "    else:\n",
    "        return inp + [token]*(maxlen - len(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordLSTMDataset(Dataset):\n",
    "    def __init__(self, words, text_df, maxlen=128):\n",
    "        self.word2ind = defaultdict(lambda:len(words))  # this is the out of vocabulary token\n",
    "        self.maxlen = maxlen           # fixed length for padding sequences\n",
    "        self.pad_ind = len(words) + 1  # a padding index so that all inputs are the same length\n",
    "        self.word2ind.update({word:i for i,word in enumerate(words)})\n",
    "        self.text_df = text_df\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        row = self.text_df.iloc[i]\n",
    "        text, label = row.content, row.label\n",
    "        word_tokens = [self.word2ind[w] for w in text.split()]\n",
    "        return torch.Tensor(pad_sequences(word_tokens, self.maxlen, self.pad_ind)).long(), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = wordLSTMDataset(words, df_train)\n",
    "dataset_val = wordLSTMDataset(words, df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, num_workers=8, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=32, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size=150, depth=1, dropout=0.3, nclasses=2, fix_emb=True,\n",
    "                 normalise=False):\n",
    "        super(WordLSTM, self).__init__()\n",
    "        \n",
    "        if normalise:\n",
    "            embedding /= np.linalg.norm(embedding,axis=1).reshape(-1, 1)\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(embedding.shape[0]+2, embedding.shape[1])\n",
    "        self.embedding.weight.data.uniform_(-0.25, 0.25)\n",
    "\n",
    "        self.embedding.weight.data[:len(embeddings)].copy_(torch.from_numpy(embeddings))\n",
    "        \n",
    "        if fix_emb:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding.shape[1], hidden_size//2, depth, dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, nclasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = self.drop(emb)\n",
    "        output, hidden = self.lstm(emb)\n",
    "        output = torch.tanh(self.bn(torch.max(output, dim=1)[0]))\n",
    "        output = self.drop(output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gallil/anaconda3/envs/test2/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = WordLSTM(embeddings)\n",
    "device = 'cuda'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, dataloader, device):\n",
    "    running_accuracy = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs = model(inputs)\n",
    "        labels = labels.to(device)\n",
    "        loss = criterion(outputs, labels.view(-1))\n",
    "\n",
    "        _, preds = outputs.max(1)\n",
    "        running_accuracy += ((preds == labels.view(-1)).sum().to(dtype=torch.float)/len(outputs)).item()\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    # loss\n",
    "    print('[%d, %5d] val loss: %.3f' %\n",
    "          (1, i + 1, running_loss / (i+1)))\n",
    "\n",
    "    # accuracy\n",
    "    print('[%d, %5d] val accuracy: %.3f' %\n",
    "          (1, i + 1, running_accuracy / (i+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.541\n",
      "[1,   200] accuracy: 0.713\n",
      "[1,   400] loss: 0.420\n",
      "[1,   400] accuracy: 0.808\n",
      "[1,    46] val loss: 0.367\n",
      "[1,    46] val accuracy: 0.833\n",
      "[2,   200] loss: 0.374\n",
      "[2,   200] accuracy: 0.830\n",
      "[2,   400] loss: 0.374\n",
      "[2,   400] accuracy: 0.838\n",
      "[1,    46] val loss: 0.355\n",
      "[1,    46] val accuracy: 0.839\n",
      "[3,   200] loss: 0.354\n",
      "[3,   200] accuracy: 0.851\n",
      "[3,   400] loss: 0.347\n",
      "[3,   400] accuracy: 0.848\n",
      "[1,    46] val loss: 0.311\n",
      "[1,    46] val accuracy: 0.867\n",
      "[4,   200] loss: 0.341\n",
      "[4,   200] accuracy: 0.848\n",
      "[4,   400] loss: 0.331\n",
      "[4,   400] accuracy: 0.853\n",
      "[1,    46] val loss: 0.319\n",
      "[1,    46] val accuracy: 0.857\n",
      "[5,   200] loss: 0.315\n",
      "[5,   200] accuracy: 0.863\n",
      "[5,   400] loss: 0.317\n",
      "[5,   400] accuracy: 0.864\n",
      "[1,    46] val loss: 0.311\n",
      "[1,    46] val accuracy: 0.854\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    running_accuracy = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader_train, 0):\n",
    "        model.train()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        labels = labels.to(device)\n",
    "        loss = criterion(outputs, labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = outputs.max(1)\n",
    "        running_accuracy += ((preds == labels.view(-1)).sum().to(dtype=torch.float)/len(outputs)).item()\n",
    "        \n",
    "        # print statistics\n",
    "        n = 200\n",
    "        running_loss += loss.item()\n",
    "        if i % n == n-1:    # print every n mini-batches\n",
    "            # loss\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / n))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            # accuracy\n",
    "            print('[%d, %5d] accuracy: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_accuracy / n))\n",
    "            running_accuracy = 0.0\n",
    "    val_metrics(model, dataloader_val, device)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), open(data_path+'_word_lstm.pth', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/aclImdb/imdb'\n",
    "model_path = base_path + '_word_lstm.pth'\n",
    "tst_path = base_path + '_test_clean.csv'\n",
    "out_path = base_path + '_test_pred_lstm.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_df = pd.read_csv(tst_path)\n",
    "tst_df = tst_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "test_sent_dataset = wordLSTMDataset(words, tst_df)\n",
    "\n",
    "# dataloaders\n",
    "valid_dataloader = DataLoader(test_sent_dataset, batch_size=32, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "prob_list = []\n",
    "\n",
    "for data in valid_dataloader:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs = model(inputs)\n",
    "        _, preds = outputs.max(1)\n",
    "        pred_list.append(preds.cpu().numpy())\n",
    "        prob_list.append(outputs.cpu().numpy())\n",
    "        \n",
    "tst_df['preds'] = np.concatenate(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8699581815729125"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tst_df['preds']==tst_df['label']).sum()/len(tst_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8699581815729125"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tst_df['preds']==tst_df['label']).sum()/len(tst_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_df.to_csv(out_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "test2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}