base_path: '../../data/mnli/mnli'   # the base path for the data, model, results, etc.
ATTACKED_INDICES: 'range(250)'  # a string describing list of the indices of the dataset to attack, can be 'range(i,j)', or '[i, j, k, m, ...]'
ENV_TYPE: 'Synonym'  # Environment type - from ['Synonym', 'SynonymDelete', 'SynonymMisspell']
USE_PPL: False  # boolean of whether to use perplexity (using GPT-2) to select more natural synonyms and integrate this for the reward
ATTACK_TYPE: 'individual'  # attack each text individually or by learning all text together or by testing a pre-trained agent - from ['individual', 'universal', 'test']
NUM_CLASSES: 3  # Number of classes, 3 for MNLI and 2 otherwise
SEED: 42  # the random seed
DEVICE: 'cuda'  # which device to use
AGENT_TYPE: 'dqn'  # agent used to attack ['dqn', 'dqn_contin']
MEMORY_SIZE: 10000  # the number of transition tuples to store in the DQN agent's memory
MODEL_TYPE: 'bert'  # attacked model type: BERT, or Word-LSTM ['bert', 'lstm']
BATCH_SIZE: 128   # number of transitions to sample for training the agent at each step
NUM_EPISODES: 100  # number of episodes to "play" for each text if attacking individually, or in test mode, or total number of rounds for universal attacks
EARLY_STOPPING: 10  # the reward above which the attack stops, set to above 200 for no early stopping. when aiming for any attack, and not necessarily the best this speeds things up
MAX_SENT_LEN: 150  # the maximum text length, which influences the number of actions. ignored if attacking individually
POLICY_UPDATE: 1  # every how many agent steps to update the policy network
TARGET_UPDATE: 5  # every how many agent episodes to update the target network
GAMMA: 0.995  # the decay parameter for the A3C loss
EPS_START: 1.0  # the initial epsilon for epsilon-greedy exploration
EPS_END: 1.0  # the minimal epsilon for epsilon-greedy exploration
EPS_DECAY: 1500  # the decay parameter for epsilon according to the equation: EPS_END + (EPS_START - EPS_END) * exp(-1. * steps_done / EPS_DECAY)
MAX_TURNS: 30000  # max number of turns the agent gets in each round
STATE_SHAPE: 768  # the size of the agent state representation
LEARNING_RATE: 0.00005  # the learning rate for the DQN agent
NORMALISE_ROUNDS: -1  # number of samples to see for state normalising parameters, -1 means no normalising, 'offline' means offline normalising (which is not supported for individual attacks)
HANDLE_OUT: 'save'  # what to do with results from ['save', 'plot'], if to save to csv or plot and show reward graph. If something else the results aren't logged in any way
MEM_TYPE:  'regular'  # one of ['priority', 'regular'] if using 'priority' a priority replay buffer is used