base_path: '../../data/aclImdb/imdb'   # the base path for the data, model, results, etc.
ATTACKED_INDICES: 'range(5)'  # 'range(166, 250)'  # a string describing list of the indices of the dataset to attack, can be 'range(i,j)', or '[i, j, k, m, ...]'
ENV_TYPE: 'Synonym'  # Environment type - from ['Synonym', 'SynonymDelete', 'SynonymMisspell']
USE_PPL: False  # boolean of whether to use perplexity (using GPT-2) to select more natural synonyms and integrate this for the reward
ATTACK_TYPE: 'universal'  # attack each text individually or by learning all text together - from ['universal', 'individual']
SEED: 42  # the random seed
DEVICE: 'cuda'  # which device to use
AGENT_TYPE: 'dqn'  # agent used to attack ['dqn', 'dqn_contin']
MODEL_TYPE: 'e2e'  # attacked model type: end-to-end transfer learning of BERT or text embedding transfer learning of BERT, or Word-LSTM ['e2e', 'transfer', 'lstm']
BATCH_SIZE: 128   # number of transitions to sample for training the agent at each step
NUM_EPISODES: 100  # number of episodes to "play" for each text if attacking individually, or altogether else
EARLY_STOPPING:   # the reward above which the attack stops, leave empty for no early_Stopping. when aiming for any attack, and not the best this speeds things up dramatically
MAX_SENT_LEN: 150  # the maximum sentence length, which influences the number of actions. ignored if attacking individually
POLICY_UPDATE: 1  # every how many agent steps to update the policy network
TARGET_UPDATE: 5  # every how many agent episodes to update the target network
GAMMA: 0.995  # the decay parameter for the A3C loss
EPS_START: 1.0  # the initial epsilon for epsilon-greedy exploration
EPS_END: 0.05  # the minimal epsilon for epsilon-greedy exploration
EPS_DECAY: 1500  # the decay parameter for epsilon according to the equation: EPS_END + (EPS_START - EPS_END) * exp(-1. * steps_done / EPS_DECAY)
MAX_TURNS: 30  # max number of turns the agent gets
STATE_SHAPE: 768  # the size of the agent state representation
LEARNING_RATE: 0.00005  # the learning rate for the DQN agent
NORMALISE_ROUNDS: -1  # number of samples to see for state normalising parameters, -1 means no normalising, 'offline' means offline normalising (Which is not supported for individual attacks)
HANDLE_OUT: 'save'  # what to do with results from ['save', 'plot'], if to save to csv or plot and show reward graph. If something else the results aren't logged in any way
MEM_TYPE:  'priority'  # one of ['priority', 'regular'] if using 'priority' a priority replay buffer is used